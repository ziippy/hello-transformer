{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d5ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_classes, num_support_per_class,\n",
    "                 vocab_size, embed_size, hidden_size,\n",
    "                 output_dim, weights):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_support = num_classes * num_support_per_class\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        if weights is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(weights))\n",
    "\n",
    "        self.bilstm = nn.LSTM(embed_size, hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(2 * hidden_size, output_dim)\n",
    "        self.fc2 = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def attention(self, x):\n",
    "        weights = torch.tanh(self.fc1(x))\n",
    "        weights = self.fc2(weights)  # (batch=k*c, seq_len, d_a)\n",
    "        batch, seq_len, d_a = weights.shape\n",
    "        weights = weights.transpose(1, 2)  # (batch=k*c, d_a, seq_len)\n",
    "        weights = weights.contiguous().view(-1, seq_len)\n",
    "        weights = F.softmax(weights, dim=1).view(batch, d_a, seq_len)\n",
    "        sentence_embeddings = torch.bmm(weights, x)  # (batch=k*c, d_a, 2*hidden)\n",
    "        avg_sentence_embeddings = torch.mean(sentence_embeddings, dim=1)  # (batch, 2*hidden)\n",
    "        return avg_sentence_embeddings\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size, _ = x.shape\n",
    "        if hidden is None:\n",
    "            h = x.data.new(2, batch_size, self.hidden_size).fill_(0).float()\n",
    "            c = x.data.new(2, batch_size, self.hidden_size).fill_(0).float()\n",
    "        else:\n",
    "            h, c = hidden\n",
    "        x = self.embedding(x)\n",
    "        outputs, _ = self.bilstm(x, (h, c))  # (batch=k*c,seq_len,2*hidden)\n",
    "        outputs = self.attention(outputs)  # (batch=k*c, 2*hidden)\n",
    "        # (c*s, 2*hidden_size), (c*q, 2*hidden_size)\n",
    "        support, query = outputs[0: self.num_support], outputs[self.num_support:]\n",
    "        # print('support, query: {} {}'.format(support.shape, query.shape))\n",
    "        return support, query\n",
    "\n",
    "\n",
    "class Induction(nn.Module):\n",
    "    def __init__(self, C, S, H, iterations):\n",
    "        super(Induction, self).__init__()\n",
    "        self.C = C\n",
    "        self.S = S\n",
    "        self.H = H\n",
    "        self.iterations = iterations\n",
    "        self.W = torch.nn.Parameter(torch.randn(H, H))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b_ij = torch.zeros(self.C, self.S).to(x)\n",
    "        for _ in range(self.iterations):\n",
    "            d_i = F.softmax(b_ij.unsqueeze(2), dim=1)  # (C,S,1)\n",
    "            a = x.reshape(-1, self.H)\n",
    "            b = torch.mm(a, self.W)\n",
    "            #print('x: {}'.format(x.shape))\n",
    "            #print(a.shape, b.shape)\n",
    "            e_ij = torch.mm(x.reshape(-1, self.H), self.W).reshape(self.C, self.S, self.H)  # (C,S,H)\n",
    "            c_i = torch.sum(d_i * e_ij, dim=1)  # (C,H)\n",
    "            # squash\n",
    "            squared = torch.sum(c_i ** 2, dim=1).reshape(self.C, -1)\n",
    "            coeff = squared / (1 + squared) / torch.sqrt(squared + 1e-9)\n",
    "            c_i = coeff * c_i\n",
    "            c_produce_e = torch.bmm(e_ij, c_i.unsqueeze(2))  # (C,S,1)\n",
    "            b_ij = b_ij + c_produce_e.squeeze(2)\n",
    "\n",
    "        return c_i\n",
    "\n",
    "\n",
    "class Relation(nn.Module):\n",
    "    def __init__(self, C, H, out_size):\n",
    "        super(Relation, self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.M = torch.nn.Parameter(torch.randn(H, H, out_size))\n",
    "        self.W = torch.nn.Parameter(torch.randn(C * out_size, C))\n",
    "        self.b = torch.nn.Parameter(torch.randn(C))\n",
    "\n",
    "    def forward(self, class_vector, query_encoder):  # (C,H) (Q,H)\n",
    "        mid_pro = []\n",
    "        for slice in range(self.out_size):\n",
    "            slice_inter = torch.mm(torch.mm(class_vector, self.M[:, :, slice]), query_encoder.transpose(1, 0))  # (C,Q)\n",
    "            mid_pro.append(slice_inter)\n",
    "        mid_pro = torch.cat(mid_pro, dim=0)  # (C*out_size,Q)\n",
    "        V = F.relu(mid_pro.transpose(0, 1))  # (Q,C*out_size)\n",
    "        probs = torch.sigmoid(torch.mm(V, self.W) + self.b)  # (Q,C)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5aebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "#from model import FewShotInduction\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "#from criterion import Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0177d",
   "metadata": {},
   "source": [
    "### 토크나이저 변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7efb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반드시 do_lower_case=True로 해야 한다.\n",
    "# bert-base-uncased는 영어 데이터를 소문자로 변환해서 학습한 모델이기 때문이다.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139fc27",
   "metadata": {},
   "source": [
    "### 데이터셋과 데이터로더 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41874a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataset():\n",
    "    def __init__(self, data_path, tokenizer, dtype):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(f'{dtype}.list', 'r') as f:\n",
    "            self.categories = [oneline.rstrip() for oneline in f]\n",
    "        self.support_dataset = {}\n",
    "        self.dataset = {}\n",
    "        for category in tqdm(self.categories, desc='reading categories'):\n",
    "            self.dataset[category] = {\n",
    "                'neg': self.get_data(category, 'neg', dtype),\n",
    "                'pos': self.get_data(category, 'pos', dtype)\n",
    "            }\n",
    "        \n",
    "        if dtype == 'test' or dtype == 'dev':\n",
    "            for category in tqdm(self.categories, desc='reading categories for support'):\n",
    "                self.support_dataset[category] = {\n",
    "                    'neg': self.get_data(category, 'neg', 'train'),\n",
    "                    'pos': self.get_data(category, 'pos', 'train'),\n",
    "                }\n",
    "        \n",
    "    def read_files(self, category, label, dtype):\n",
    "        data = {\n",
    "            'text': [],\n",
    "            'label': []\n",
    "        }\n",
    "        for t in ['t2', 't4', 't5']:\n",
    "            filename = f'{category}.{t}.{dtype}'\n",
    "            with open(os.path.join(self.data_path, filename), 'r', encoding='utf-8') as f:\n",
    "                for oneline in f:\n",
    "                    oneline = oneline.rstrip()\n",
    "                    text = oneline[:-2]\n",
    "                    if int(oneline[-2:]) == 1 and label == 'pos':\n",
    "                        tensor = self.tokenizer(text, return_tensors='pt')\n",
    "                        data['text'].append(tensor['input_ids'][0])\n",
    "                        data['label'].append(1)\n",
    "                    elif int(oneline[-2:]) == -1 and label == 'neg':\n",
    "                        tensor = self.tokenizer(text, return_tensors='pt')\n",
    "                        data['text'].append(tensor['input_ids'][0])\n",
    "                        data['label'].append(0)\n",
    "        data['label'] = torch.tensor(data['label'])\n",
    "        return data\n",
    "    \n",
    "    def get_data(self, category, label, dtype):\n",
    "        data = self.read_files(category, label, dtype)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32053c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Amazon_few_shot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab1e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading categories:   0%|                                                                        | 0/14 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "reading categories: 100%|███████████████████████████████████████████████████████████████| 14/14 [02:40<00:00, 11.46s/it]\n",
      "reading categories: 100%|█████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.56it/s]\n",
      "reading categories for support: 100%|█████████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.49s/it]\n",
      "reading categories: 100%|█████████████████████████████████████████████████████████████████| 4/4 [00:17<00:00,  4.48s/it]\n",
      "reading categories for support: 100%|█████████████████████████████████████████████████████| 4/4 [00:00<00:00, 17.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AmazonDataset(data_path, tokenizer, 'train')\n",
    "dev_dataset = AmazonDataset(data_path, tokenizer, 'dev')\n",
    "test_dataset = AmazonDataset(data_path, tokenizer, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac6bf606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(a_text, b_text):\n",
    "    a_text_len = a_text.shape[1]\n",
    "    b_text_len = b_text.shape[1]\n",
    "\n",
    "    if a_text_len > b_text_len:\n",
    "        b_text = torch.cat([b_text, torch.zeros(b_text.shape[0], a_text_len-b_text_len).long()], dim=1)\n",
    "    else:\n",
    "        a_text = torch.cat([a_text, torch.zeros(a_text.shape[0], b_text_len-a_text_len).long()], dim=1)\n",
    "        \n",
    "    return a_text, b_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2271effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataLoader():\n",
    "    def __init__(self, dataset, batch_size, n_support):\n",
    "        assert n_support % 2 == 0, 'n_support should be multiple of 2'\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.n_support = n_support\n",
    "        self.neg_idx = {k:0 for k in dataset.dataset}\n",
    "        self.pos_idx = {k:0 for k in dataset.dataset}\n",
    "        self.neg_len = {k:len(dataset.dataset[k]['neg']['text']) for k in dataset.dataset}\n",
    "        self.pos_len = {k:len(dataset.dataset[k]['pos']['text']) for k in dataset.dataset}\n",
    "        self.neg = {k:dataset.dataset[k]['neg'] for k in dataset.dataset}\n",
    "        self.pos = {k:dataset.dataset[k]['pos'] for k in dataset.dataset}\n",
    "        self.idx = 0\n",
    "        self.categories = [k for k in dataset.dataset]\n",
    "        \n",
    "        # prepare for test dataset, support dataset should come from \"*.train\"\n",
    "        self.neg_support_idx = {}\n",
    "        self.pos_support_idx = {}\n",
    "        self.neg_support_len = {}\n",
    "        self.pos_support_len = {}\n",
    "        if self.dataset.support_dataset:\n",
    "            self.neg_support_idx = {k:0 for k in self.dataset.support_dataset}\n",
    "            self.pos_support_idx = {k:0 for k in self.dataset.support_dataset}\n",
    "            self.neg_support_len = {k:len(self.dataset.support_dataset[k]['neg']['text']) for k in self.dataset.support_dataset}\n",
    "            self.pos_support_len = {k:len(self.dataset.support_dataset[k]['pos']['text']) for k in self.dataset.support_dataset}\n",
    "        \n",
    "    def get_batch(self):\n",
    "        category = self.categories[self.idx % len(self.categories)]\n",
    "        neg = self.neg[category]\n",
    "        pos = self.pos[category]\n",
    "        neg_start_idx = self.neg_idx[category] % self.neg_len[category]\n",
    "        pos_start_idx = self.pos_idx[category] % self.pos_len[category]\n",
    "        \n",
    "        # prepare negative/positive dataset\n",
    "        neg_text = neg['text'][neg_start_idx:neg_start_idx+(self.batch_size//2)]\n",
    "        pos_text = pos['text'][pos_start_idx:pos_start_idx+(self.batch_size//2)]\n",
    "        neg_label = neg['label'][neg_start_idx:neg_start_idx+(self.batch_size//2)]\n",
    "        pos_label = pos['label'][pos_start_idx:pos_start_idx+(self.batch_size//2)]\n",
    "        self.neg_idx[category] += (self.batch_size//2)\n",
    "        self.pos_idx[category] += (self.batch_size//2)\n",
    "        \n",
    "        if len(neg_text) + len(pos_text) != self.batch_size:\n",
    "            return self.get_batch()\n",
    "            \n",
    "        # padding text dataset\n",
    "        neg_text = pad_sequence([n for n in neg_text], batch_first=True)\n",
    "        pos_text = pad_sequence([p for p in pos_text], batch_first=True)\n",
    "        neg_text, pos_text = pad_text(neg_text, pos_text)\n",
    "            \n",
    "        # prepare support/query text\n",
    "        neg_support_text = neg_text[:self.n_support//2]\n",
    "        pos_support_text = pos_text[:self.n_support//2]\n",
    "        neg_query_text = neg_text[self.n_support//2:]\n",
    "        pos_query_text = pos_text[self.n_support//2:]\n",
    "        \n",
    "        # prepare support/query label\n",
    "        neg_support_label = neg_label[:self.n_support//2]\n",
    "        pos_support_label = pos_label[:self.n_support//2]\n",
    "        neg_query_label = neg_label[self.n_support//2:]\n",
    "        pos_query_label = pos_label[self.n_support//2:]\n",
    "        \n",
    "        # merge support/query text\n",
    "        support_text = torch.cat([neg_support_text, pos_support_text], dim=0)\n",
    "        query_text = torch.cat([neg_query_text, pos_query_text], dim=0)\n",
    "        \n",
    "        # merge support/query label\n",
    "        support_label = torch.cat([neg_support_label, pos_support_label], dim=0)\n",
    "        query_label = torch.cat([neg_query_label, pos_query_label], dim=0)\n",
    "        \n",
    "        # make data and label\n",
    "        data = torch.cat([support_text, query_text], dim=0)\n",
    "        label = torch.cat([support_label, query_label], dim=0)\n",
    "        \n",
    "        # increase category index\n",
    "        self.idx += 1\n",
    "        return data, label\n",
    "    \n",
    "    def get_batch_test(self):\n",
    "        assert self.dataset.support_dataset, 'support_dataset is empty'\n",
    "        \n",
    "        category = self.categories[self.idx % len(self.categories)]\n",
    "        neg = self.neg[category]\n",
    "        pos = self.pos[category]\n",
    "        neg_query_start_idx = self.neg_idx[category] % self.neg_len[category]\n",
    "        pos_query_start_idx = self.pos_idx[category] % self.pos_len[category]\n",
    "        neg_support_start_idx = self.neg_support_idx[category] % self.neg_support_len[category]\n",
    "        pos_support_start_idx = self.pos_support_idx[category] % self.pos_support_len[category]\n",
    "        \n",
    "        # prepare negative/positive support dataset from support_dataset\n",
    "        category_suuport_dataset = self.dataset.support_dataset[category]\n",
    "        neg_support_text = category_suuport_dataset['neg']['text'][neg_support_start_idx:neg_support_start_idx+self.n_support//2]\n",
    "        pos_support_text = category_suuport_dataset['pos']['text'][pos_support_start_idx:pos_support_start_idx+self.n_support//2]\n",
    "        neg_support_label = category_suuport_dataset['neg']['label'][neg_support_start_idx:neg_support_start_idx+self.n_support//2]\n",
    "        pos_support_label = category_suuport_dataset['pos']['label'][pos_support_start_idx:pos_support_start_idx+self.n_support//2]\n",
    "        self.neg_support_idx[category] += (self.n_support//2)\n",
    "        self.pos_support_idx[category] += (self.n_support//2)\n",
    "        \n",
    "        # prepare negative/positive query dataset\n",
    "        neg_query_text = neg['text'][neg_query_start_idx:neg_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        pos_query_text = pos['text'][pos_query_start_idx:pos_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        neg_query_label = neg['label'][neg_query_start_idx:neg_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        pos_query_label = pos['label'][pos_query_start_idx:pos_query_start_idx+(self.batch_size//2 - self.n_support//2)]\n",
    "        self.neg_idx[category] += (self.batch_size//2 - self.n_support//2)\n",
    "        self.pos_idx[category] += (self.batch_size//2 - self.n_support//2)\n",
    "        \n",
    "        # padding support text dataset\n",
    "        if self.n_support:\n",
    "            neg_support_text = pad_sequence([n for n in neg_support_text], batch_first=True)\n",
    "            pos_support_text = pad_sequence([n for n in pos_support_text], batch_first=True)\n",
    "            neg_support_text, pos_support_text = pad_text(neg_support_text, pos_support_text)\n",
    "        else:\n",
    "            neg_support_text = torch.tensor([[]])\n",
    "            pos_support_text = torch.tensor([[]])\n",
    "            \n",
    "        # padding text dataset\n",
    "        neg_query_text = pad_sequence([n for n in neg_query_text], batch_first=True)\n",
    "        pos_query_text = pad_sequence([p for p in pos_query_text], batch_first=True)\n",
    "        neg_query_text, pos_query_text = pad_text(neg_query_text, pos_query_text)\n",
    "\n",
    "        # concatenating support/query text dataset\n",
    "        support_text = torch.cat([neg_support_text, pos_support_text], dim=0)\n",
    "        query_text = torch.cat([neg_query_text, pos_query_text], dim=0)\n",
    "        support_text, query_text = pad_text(support_text, query_text)\n",
    "\n",
    "        # make final data and label\n",
    "        if self.n_support:\n",
    "            data = torch.cat([support_text, query_text], dim=0)\n",
    "        else:\n",
    "            data = query_text\n",
    "        label = torch.cat([neg_support_label, pos_support_label, neg_query_label, pos_query_label], dim=0)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97b2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b09877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = AmazonDataLoader(train_dataset, batch_size=64, n_support=support*2)\n",
    "dev_dataloader = AmazonDataLoader(dev_dataset, batch_size=64, n_support=support*2)\n",
    "test_dataloader = AmazonDataLoader(test_dataset, batch_size=64, n_support=support*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f6b30a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 149]) tensor(0.5000)\n",
      "torch.Size([64, 460]) tensor(0.5000)\n",
      "torch.Size([64, 254]) tensor(0.5000)\n",
      "torch.Size([64, 262]) tensor(0.5000)\n",
      "torch.Size([64, 1283]) tensor(0.5000)\n",
      "torch.Size([64, 1658]) tensor(0.5000)\n",
      "torch.Size([64, 613]) tensor(0.5000)\n",
      "torch.Size([64, 359]) tensor(0.5000)\n",
      "torch.Size([64, 530]) tensor(0.5000)\n",
      "torch.Size([64, 602]) tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d, l = train_dataloader.get_batch()\n",
    "    print(d.shape, l.float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f653ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 327]) tensor(0.5000)\n",
      "torch.Size([55, 181]) tensor(0.5818)\n",
      "torch.Size([64, 198]) tensor(0.5000)\n",
      "torch.Size([46, 295]) tensor(0.6957)\n",
      "torch.Size([64, 197]) tensor(0.5000)\n",
      "torch.Size([64, 276]) tensor(0.5000)\n",
      "torch.Size([55, 186]) tensor(0.5818)\n",
      "torch.Size([64, 270]) tensor(0.5000)\n",
      "torch.Size([26, 130]) tensor(0.4615)\n",
      "torch.Size([64, 327]) tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d, l = dev_dataloader.get_batch_test()\n",
    "    print(d.shape, l.float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea86511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 743]) tensor(0.5000)\n",
      "torch.Size([64, 841]) tensor(0.5000)\n",
      "torch.Size([64, 1386]) tensor(0.5000)\n",
      "torch.Size([64, 706]) tensor(0.5000)\n",
      "torch.Size([64, 1026]) tensor(0.5000)\n",
      "torch.Size([64, 1126]) tensor(0.5000)\n",
      "torch.Size([64, 1116]) tensor(0.5000)\n",
      "torch.Size([64, 1333]) tensor(0.5000)\n",
      "torch.Size([64, 568]) tensor(0.5000)\n",
      "torch.Size([64, 570]) tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    d, l = test_dataloader.get_batch_test()\n",
    "    print(d.shape, l.float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e25b2c",
   "metadata": {},
   "source": [
    "### 아마존 데이터셋에 대한 메타러닝 모델 클래스 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86929327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotInduction(nn.Module):\n",
    "    def __init__(self, C, S, vocab_size, embed_size, hidden_size, d_a,\n",
    "                 iterations, outsize, weights=None):\n",
    "        super(FewShotInduction, self).__init__()\n",
    "        self.encoder = Encoder(C, S, vocab_size, embed_size, hidden_size, d_a, weights)\n",
    "        self.induction = Induction(C, S, 2 * hidden_size, iterations)\n",
    "        self.relation = Relation(C, 2 * hidden_size, outsize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        support_encoder, query_encoder = self.encoder(x)  # (k*c, 2*hidden_size)\n",
    "        # print('** support_encoder: {}'.format(support_encoder.shape))\n",
    "        # print('** query_encoder: {}'.format(query_encoder.shape))\n",
    "        class_vector = self.induction(support_encoder)\n",
    "        # print('** class_vector: {}'.format(class_vector.shape))\n",
    "        probs = self.relation(class_vector, query_encoder)\n",
    "        # print('** relation: {}'.format(probs.shape))\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16375bfd",
   "metadata": {},
   "source": [
    "### 아마존 데이터셋에 대한 메타러닝 모델 객체 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0ada21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FewShotInduction(C=2,\n",
    "                         S=support,\n",
    "                         vocab_size=len(tokenizer),\n",
    "                         embed_size=300,\n",
    "                         hidden_size=128,\n",
    "                         d_a=64,\n",
    "                         iterations=3,\n",
    "                         outsize=100)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "028c86cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2288770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=float(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67c7fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "\n",
    "class Criterion(_Loss):\n",
    "    def __init__(self, way=2, shot=5):\n",
    "        super(Criterion, self).__init__()\n",
    "        self.amount = way * shot\n",
    "\n",
    "    def forward(self, probs, target, return_pred_label=False):  # (Q,C) (Q)\n",
    "        target = target[self.amount:]\n",
    "        target_onehot = torch.zeros_like(probs)\n",
    "        #print('** sum of probs/target_onehot: {} {}'.format(probs.sum(), target_onehot.sum()))\n",
    "        target_onehot = target_onehot.scatter(1, target.reshape(-1, 1), 1)\n",
    "        loss = torch.mean((probs - target_onehot) ** 2)\n",
    "        pred = torch.argmax(probs, dim=1)\n",
    "        acc = torch.sum(target == pred).float() / target.shape[0]\n",
    "        #print('** acc: {}'.format(acc))\n",
    "\n",
    "        if return_pred_label:\n",
    "            return loss, acc, pred, target\n",
    "        else:\n",
    "            return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c398c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Criterion(way=2, shot=support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3871d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episode):\n",
    "    model.train()\n",
    "    data, target = train_dataloader.get_batch()\n",
    "    if torch.cuda.is_available():\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    predict = model(data)\n",
    "    loss, acc = criterion(predict, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8db20a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev(episode):\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    count = 0.\n",
    "    for i in range(100):\n",
    "        data, target = dev_dataloader.get_batch_test()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        predict = model(data)\n",
    "        _, acc = criterion(predict, target)\n",
    "        amount = len(target) - support * 2\n",
    "        correct += acc * amount\n",
    "        count += amount\n",
    "    acc = correct / count\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5cef811",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_interval = 100\n",
    "best_acc = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe8f8bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏              | 101/9999 [00:24<1:31:51,  1.80it/s, loss=tensor(0.5075, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.4788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████▌         | 3300/9999 [11:41<1:22:08,  1.36it/s, loss=tensor(0.4568, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████▉          | 3701/9999 [13:07<53:54,  1.95it/s, loss=tensor(0.3759, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.5974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████          | 3801/9999 [13:28<51:22,  2.01it/s, loss=tensor(0.2858, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|██████▌         | 4101/9999 [14:32<57:20,  1.71it/s, loss=tensor(0.4718, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████▋         | 4201/9999 [14:53<48:55,  1.98it/s, loss=tensor(0.1853, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████▉         | 4301/9999 [15:15<54:29,  1.74it/s, loss=tensor(0.5544, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████         | 4401/9999 [15:37<53:19,  1.75it/s, loss=tensor(0.2900, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████▎        | 4600/9999 [16:19<55:27,  1.62it/s, loss=tensor(0.2697, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████▌       | 4700/9999 [16:41<1:02:38,  1.41it/s, loss=tensor(0.1854, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████▋       | 4800/9999 [17:03<1:04:21,  1.35it/s, loss=tensor(0.1468, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████▊        | 4901/9999 [17:25<49:21,  1.72it/s, loss=tensor(0.2268, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.6876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████        | 5001/9999 [17:46<46:35,  1.79it/s, loss=tensor(0.1576, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████▌      | 6001/9999 [21:19<33:06,  2.01it/s, loss=tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████▉      | 6200/9999 [22:02<47:12,  1.34it/s, loss=tensor(0.2214, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████▏     | 6401/9999 [22:44<29:38,  2.02it/s, loss=tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████▋     | 6700/9999 [23:48<33:11,  1.66it/s, loss=tensor(0.1802, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████     | 6901/9999 [24:31<25:56,  1.99it/s, loss=tensor(0.1562, device='cuda:0', grad_fn=<MeanBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better acc! Saving model! -> 0.7327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████| 9999/9999 [35:32<00:00,  4.69it/s, loss=tensor(0.2797, device='cuda:0', grad_fn=<MeanBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "tbar = tqdm(range(1, 10000))\n",
    "for episode in tbar:\n",
    "    \n",
    "    loss = train(episode)\n",
    "    if episode % dev_interval == 0:\n",
    "        acc = dev(episode)\n",
    "        if acc > best_acc:\n",
    "            print('Better acc! Saving model! -> {:.4f}'.format(acc))\n",
    "            best_acc = acc\n",
    "    tbar.set_postfix(loss=loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44a2897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'fewshot_model_{support}.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2850445c",
   "metadata": {},
   "source": [
    "### 학습된 메타러닝 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c68249a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 5\n",
    "criterion = Criterion(way=2, shot=support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d18232a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FewShotInduction(C=2,\n",
    "                         S=support,\n",
    "                         vocab_size=len(tokenizer),\n",
    "                         embed_size=300,\n",
    "                         hidden_size=128,\n",
    "                         d_a=64,\n",
    "                         iterations=3,\n",
    "                         outsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82df0371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load(f'./fewshot_model_{support}.bin', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37e4768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d18d8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    count = 0.\n",
    "    for i in range(100):\n",
    "        data, target = test_dataloader.get_batch_test()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        predict = model(data)\n",
    "        _, acc = criterion(predict, target)\n",
    "        amount = len(target) - support * 2\n",
    "        correct += acc * amount\n",
    "        count += amount\n",
    "        \n",
    "    acc = correct / count\n",
    "    print('Test Acc: {}'.format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a6856ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.6590113639831543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6590, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
