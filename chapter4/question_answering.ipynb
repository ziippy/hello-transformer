{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595cd037",
   "metadata": {},
   "source": [
    "## 질의응답 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28a784",
   "metadata": {},
   "source": [
    "질의응답 데이터셋으로 유명한 SQuAD (Stanford Question Answering Dataset) 사용 예정\n",
    "1.1 버전과 2.0 버전중에서 2.0 버전을 사용하려고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4211b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98d1e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523d8698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "# [CLS] who was jim henson ? [SEP] jim henson was a nice puppet [SEP]\n",
    "#   0    1   2   3   4     5   6    7   8      9  10  11   12     13\n",
    "# 정답의 start_position 은 10, end_position 은 12\n",
    "\n",
    "input_ids = [[101, 2040, 2001, 3958, 27227, 1029, 102, 3958, 27227, 2001, 1037, 3835, 13997, 102]]\n",
    "token_type_ids = [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]\n",
    "attention_mask = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb4ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'input_ids':torch.tensor(input_ids),\n",
    "    'token_type_ids':torch.tensor(token_type_ids),\n",
    "    'attention_mask':torch.tensor(attention_mask)\n",
    "}\n",
    "start_position = torch.tensor([[10]])\n",
    "end_position = torch.tensor([[12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f245c1",
   "metadata": {},
   "source": [
    "## SQuAD 데이터셋 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2fcbb",
   "metadata": {},
   "source": [
    "위에서 예로 들었던 input 이 SQuAD 데이터셋 형식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b74fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be101fbd",
   "metadata": {},
   "source": [
    "### 토크나이저 변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d886f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554ee79",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91799477",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82997ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv *-v2.0* squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99fb91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 doc_span_index,\n",
    "                 tokens,\n",
    "                 token_to_orig_map,\n",
    "                 token_is_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 cls_index,\n",
    "                 p_mask,\n",
    "                 paragraph_len,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 is_impossible=None):\n",
    "        self.unique_id = unique_id\n",
    "        self.example_index = example_index\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.cls_index = cls_index\n",
    "        self.p_mask = p_mask\n",
    "        self.paragraph_len = paragraph_len\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n",
    "        \n",
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
    "                         orig_answer_text):\n",
    "    \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
    "\n",
    "    # The SQuAD annotations are character based. We first project them to\n",
    "    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n",
    "    # often find a \"better match\". For example:\n",
    "    #\n",
    "    #   Question: What year was John Smith born?\n",
    "    #   Context: The leader was John Smith (1895-1943).\n",
    "    #   Answer: 1895\n",
    "    #\n",
    "    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
    "    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
    "    # the exact answer, 1895.\n",
    "    #\n",
    "    # However, this is not always possible. Consider the following:\n",
    "    #\n",
    "    #   Question: What country is the top exporter of electornics?\n",
    "    #   Context: The Japanese electronics industry is the lagest in the world.\n",
    "    #   Answer: Japan\n",
    "    #\n",
    "    # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
    "    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
    "    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
    "    # in SQuAD, but does happen.\n",
    "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "\n",
    "    for new_start in range(input_start, input_end + 1):\n",
    "        for new_end in range(input_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
    "            if text_span == tok_answer_text:\n",
    "                return (new_start, new_end)\n",
    "\n",
    "    return (input_start, input_end)\n",
    "\n",
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single\n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
    "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=0, pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    unique_id = 1000000000\n",
    "    # cnt_pos, cnt_neg = 0, 0\n",
    "    # max_N, max_M = 1024, 1024\n",
    "    # f = np.zeros((max_N, max_M), dtype=np.float32)\n",
    "\n",
    "    do_print = False\n",
    "    features = []\n",
    "    for (example_index, example) in enumerate(trange(len(examples))):\n",
    "\n",
    "        # if example_index % 100 == 0:\n",
    "        #     logger.info('Converting %s/%s pos %s neg %s', example_index, len(examples), cnt_pos, cnt_neg)\n",
    "\n",
    "        example = examples[example_index]\n",
    "        query_tokens = tokenizer.tokenize(example.question)\n",
    "\n",
    "        # 쿼리 길이가 길 경우 max_query_length로 자르기\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "        # example.context: [The, Normans, ...]\n",
    "        # token: The, Normans, ...\n",
    "        # subtoken: the, norman, ##s\n",
    "        # tok_to_orig_index: 0, 1, 1, ...\n",
    "        # orig_to_tok_index: 0, 2, ...\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = []\n",
    "        for (i, token) in enumerate(example.context):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "        if is_training and example.is_impossible:\n",
    "            tok_start_position = -1\n",
    "            tok_end_position = -1\n",
    "        if is_training and not example.is_impossible:\n",
    "            tok_start_position = orig_to_tok_index[example.start]\n",
    "            if example.end < len(example.context) - 1:\n",
    "                tok_end_position = orig_to_tok_index[example.end + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "            (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "                example.answer)\n",
    "\n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "\n",
    "        if len(doc_spans) == 2:\n",
    "            do_print = True\n",
    "\n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = []\n",
    "            token_to_orig_map = {}\n",
    "            token_is_max_context = {}\n",
    "            segment_ids = []\n",
    "\n",
    "            # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
    "            # Original TF implem also keep the classification token (set to 0) (not sure why...)\n",
    "            p_mask = []\n",
    "\n",
    "            # CLS token at the beginning\n",
    "            if not cls_token_at_end:\n",
    "                tokens.append(cls_token)\n",
    "                segment_ids.append(cls_token_segment_id)\n",
    "                p_mask.append(0)\n",
    "                cls_index = 0\n",
    "\n",
    "            # Query\n",
    "            for token in query_tokens:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(sequence_a_segment_id)\n",
    "                p_mask.append(1)\n",
    "\n",
    "            # SEP token\n",
    "            tokens.append(sep_token)\n",
    "            segment_ids.append(sequence_a_segment_id)\n",
    "            p_mask.append(1)\n",
    "\n",
    "            # Paragraph\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(sequence_b_segment_id)\n",
    "                p_mask.append(0)\n",
    "            paragraph_len = doc_span.length\n",
    "\n",
    "            # SEP token\n",
    "            tokens.append(sep_token)\n",
    "            segment_ids.append(sequence_b_segment_id)\n",
    "            p_mask.append(1)\n",
    "\n",
    "            # CLS token at the end\n",
    "            if cls_token_at_end:\n",
    "                tokens.append(cls_token)\n",
    "                segment_ids.append(cls_token_segment_id)\n",
    "                p_mask.append(0)\n",
    "                cls_index = len(tokens) - 1  # Index of classification token\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(pad_token)\n",
    "                input_mask.append(0 if mask_padding_with_zero else 1)\n",
    "                segment_ids.append(pad_token_segment_id)\n",
    "                p_mask.append(1)\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            # start_position의 위치가 doc_span의 길이 전체를 넘어버릴 경우 is_impossible=True로 수정하기\n",
    "            span_is_impossible = example.is_impossible\n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            if is_training and not span_is_impossible:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                out_of_span = False\n",
    "                if not (tok_start_position >= doc_start and\n",
    "                        tok_end_position <= doc_end):\n",
    "                    out_of_span = True\n",
    "                if out_of_span:\n",
    "                    start_position = 0\n",
    "                    end_position = 0\n",
    "                    span_is_impossible = True\n",
    "                else:\n",
    "                    doc_offset = len(query_tokens) + 2\n",
    "                    start_position = tok_start_position - doc_start + doc_offset\n",
    "                    end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "            if is_training and span_is_impossible:\n",
    "                start_position = cls_index\n",
    "                end_position = cls_index\n",
    "\n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=unique_id,\n",
    "                    example_index=example_index,\n",
    "                    doc_span_index=doc_span_index,\n",
    "                    tokens=tokens,\n",
    "                    token_to_orig_map=token_to_orig_map,\n",
    "                    token_is_max_context=token_is_max_context,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    cls_index=cls_index,\n",
    "                    p_mask=p_mask,\n",
    "                    paragraph_len=paragraph_len,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                    is_impossible=span_is_impossible))\n",
    "            unique_id += 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8889fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import collections\n",
    "\n",
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class SquadExample():\n",
    "    def __init__(self, qid, context, question, answer, start, end, is_impossible):\n",
    "        self.qid = qid\n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.is_impossible = is_impossible\n",
    "        \n",
    "    def __repr__(self):\n",
    "        #return self.context[self.start:self.end]\n",
    "        #if self.context[self.start:self.end] != self.answer:\n",
    "        #    return 'NA!! {} - {}'.format(self.context[self.start:self.end], answer)\n",
    "        return 'id:{}  question:{}...  answer:{}...  is_impossible:{}'.format(\n",
    "            self.qid,\n",
    "            self.question[:10],\n",
    "            self.answer[:10],\n",
    "            self.is_impossible)\n",
    "\n",
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, is_train=True, is_inference=False):\n",
    "        '''\n",
    "        path: SquadDataset 데이터셋 위치\n",
    "        tokenizer: Squad 데이터셋을 토크나이징할 토크나이저, ex) BertTokenizer\n",
    "        is_train: SquadDataset을 정의하는 목적이 모델 학습용일 경우 True, 그렇지 않으면 False\n",
    "        is_inference: SquadDataset을 정의하는 목적이 인퍼런스용일 경우 True, 그렇지 않으면 False\n",
    "        '''\n",
    "        \n",
    "        if is_train:\n",
    "            filename = os.path.join(path, 'train-v2.0.json')\n",
    "        else:\n",
    "            if is_inference:\n",
    "                filename = os.path.join(path, 'test-v2.0.json')\n",
    "            else:\n",
    "                filename = os.path.join(path, 'dev-v2.0.json')\n",
    "\n",
    "        cached_features_file = os.path.join(os.path.dirname(filename), 'cached_{}_64.cache'.format('train' if is_train else 'valid'))\n",
    "        #cached_examples_file = os.path.join(os.path.dirname(filename), 'cached_example_{}_64.cache'.format('train' if is_train else 'valid'))\n",
    "\n",
    "        if os.path.exists(cached_features_file):\n",
    "            print('cache file exists')\n",
    "            #self.examples = torch.load(cached_examples_file)\n",
    "            self.features = torch.load(cached_features_file)\n",
    "        else:\n",
    "            print('cache file does not exist')\n",
    "\n",
    "            with open(filename, \"r\", encoding='utf-8') as reader:\n",
    "                input_data = json.load(reader)[\"data\"]\n",
    "                \n",
    "            example_count = 0\n",
    "\n",
    "            self.examples = []\n",
    "            for entry in input_data:\n",
    "                for paragraph in entry[\"paragraphs\"]:\n",
    "                    context = paragraph['context']\n",
    "                    \n",
    "                    doc_tokens = []\n",
    "                    char_to_word_offset = []\n",
    "                    prev_is_whitespace = True\n",
    "                    for c in context:\n",
    "                        if is_whitespace(c):\n",
    "                            prev_is_whitespace = True\n",
    "                        else:\n",
    "                            if prev_is_whitespace:\n",
    "                                doc_tokens.append(c)\n",
    "                            else:\n",
    "                                doc_tokens[-1] += c\n",
    "                            prev_is_whitespace = False\n",
    "                        char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "                            \n",
    "                            \n",
    "                    for qa in paragraph['qas']:\n",
    "                        is_impossible = qa['is_impossible']\n",
    "                        \n",
    "                        if not is_impossible:\n",
    "                            answer = qa['answers'][0]\n",
    "                            original_answer = answer['text']\n",
    "                            answer_start = answer['answer_start']\n",
    "                            \n",
    "                            answer_length = len(original_answer)\n",
    "                            start_pos = char_to_word_offset[answer_start]\n",
    "                            end_pos = char_to_word_offset[answer_start + answer_length - 1]\n",
    "\n",
    "                            answer_end = answer_start + len(original_answer)\n",
    "                        else:\n",
    "                            original_answer = ''\n",
    "                            start_pos = 1\n",
    "                            end_pos = -1\n",
    "\n",
    "                        example = SquadExample(\n",
    "                            qid=qa['id'],\n",
    "                            context=doc_tokens,\n",
    "                            question=qa['question'],\n",
    "                            answer=original_answer,\n",
    "                            start=start_pos,\n",
    "                            end=end_pos,\n",
    "                            is_impossible=is_impossible)\n",
    "                        self.examples.append(example)\n",
    "                        \n",
    "                        example_count += 1\n",
    "                        if example_count >= 500:\n",
    "                            break\n",
    "                    if example_count >= 500:\n",
    "                            break\n",
    "                if example_count >= 500:\n",
    "                            break\n",
    "            print('examples: {}'.format(len(self.examples)))\n",
    "\n",
    "            self.features = convert_examples_to_features(\n",
    "                examples=self.examples,\n",
    "                tokenizer=tokenizer,\n",
    "                max_seq_length=384,\n",
    "                doc_stride=128,\n",
    "                max_query_length=64,\n",
    "                is_training=True if not is_inference else False)\n",
    "            print('is_training: {}'.format(True if not is_inference else False))\n",
    "\n",
    "            # torch.save(self.examples, cached_examples_file)\n",
    "            torch.save(self.features, cached_features_file)\n",
    "\n",
    "        '''\n",
    "        # Convert to Tensors and build dataset\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in self.features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in self.features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in self.features], dtype=torch.long)\n",
    "        all_cls_index = torch.tensor([f.cls_index for f in self.features], dtype=torch.long)\n",
    "        all_p_mask = torch.tensor([f.p_mask for f in self.features], dtype=torch.float)\n",
    "        if is_train:\n",
    "            all_start_positions = torch.tensor([f.start_position for f in self.features], dtype=torch.long)\n",
    "            all_end_positions = torch.tensor([f.end_position for f in self.features], dtype=torch.long)\n",
    "            dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                                    all_start_positions, all_end_positions,\n",
    "                                    all_cls_index, all_p_mask)\n",
    "        else:\n",
    "            all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "            dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index, all_cls_index, all_p_mask)\n",
    "        return dataset\n",
    "        '''\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb64605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf squad/*.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82a4c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache file does not exist\n",
      "examples: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:02<00:00, 219.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_training: True\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SquadDataset('squad', tokenizer, is_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4edf4",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ff9307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, is_inference=False, shuffle=True):\n",
    "        '''\n",
    "        dataset: SquadDataset으로 정의한 데이터셋 객체\n",
    "        batch_size: 배치 사이즈\n",
    "        is_inference: SquadDataLoader를 인퍼런스 목적으로 사용할 경우 True, 그렇지 않으면 False\n",
    "        shuffle: 데이터의 순서를 섞을 경우 True, 그렇지 않으면 False\n",
    "        '''\n",
    "        self.is_inference = is_inference\n",
    "        super().__init__(dataset, collate_fn=self.squad_collate_fn, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "    def squad_collate_fn(self, features):\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "        all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
    "        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
    "\n",
    "        # return 6 tensors\n",
    "        if self.is_inference:\n",
    "            all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "            return all_input_ids, all_input_mask, all_segment_ids, all_cls_index, all_p_mask, all_example_index\n",
    "        # return 7 tensors\n",
    "        else:\n",
    "            all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
    "            all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
    "            return all_input_ids, all_input_mask, all_segment_ids, all_cls_index, all_p_mask, all_start_positions, all_end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43fd66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataloader = SquadDataLoader(train_dataset, batch_size=16, is_inference=False, shuffle=True)\n",
    "train_dataloader = SquadDataLoader(train_dataset, batch_size=4, is_inference=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2ac0137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4, 384]) torch.Size([4]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    if i > 10:\n",
    "        break\n",
    "    input_ids, input_mask, segment_ids, cls_index, p_mask, start_positions, end_positions = batch\n",
    "    print(input_ids.shape, input_mask.shape, segment_ids.shape, cls_index.shape, p_mask.shape, start_positions.shape, end_positions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0f274",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8da6847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7842b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ba7241f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5afb6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer와 Loss 함수는 가장 일반적인 것으로 정의했다.\n",
    "# 이 노트북 파일의 목적은 BERT를 이용해서 높은 성능의 모델을 간편하게 만들 수 있다는 것을 보여주기 위함이다.\n",
    "# Optimizer와 Loss를 최적화할 경우 좋은 성능이 나온 이유를 잘 설명할 수 없다.\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45571731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    tbar = tqdm(dataloader, desc='Training', leave=True)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for i, batch in enumerate(tbar):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # cls_index와 p_mask는 XLNet 모델에 사용되므로 BERT에서는 사용하지 않는다.\n",
    "        input_ids, input_mask, segment_ids, cls_index, p_mask, start_positions, end_positions = batch\n",
    "        \n",
    "        # to cuda\n",
    "        if torch.cuda.is_available():\n",
    "            input_ids = input_ids.cuda()\n",
    "            input_mask = input_mask.cuda()\n",
    "            segment_ids = segment_ids.cuda()\n",
    "            start_positions = start_positions.cuda()\n",
    "            end_positions = end_positions.cuda()\n",
    "            \n",
    "        #if torch.backends.mps.is_available():\n",
    "        #    input_ids = input_ids.to(\"mps\")\n",
    "        \n",
    "        # train model\n",
    "        #out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'token_type_ids': segment_ids,\n",
    "            'attention_mask': input_mask,\n",
    "        }\n",
    "        out = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = out.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data.item()\n",
    "        tbar.set_description(\"Average Loss = {:.4f})\".format(total_loss/(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae084216",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5e7a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 4.1722): 100%|█████████████████████████████████████████████████████████| 132/132 [04:02<00:00,  1.84s/it]\n",
      "Average Loss = 2.0461): 100%|█████████████████████████████████████████████████████████| 132/132 [04:02<00:00,  1.84s/it]\n",
      "Average Loss = 1.0060): 100%|█████████████████████████████████████████████████████████| 132/132 [04:02<00:00,  1.84s/it]\n",
      "Average Loss = 0.6203): 100%|█████████████████████████████████████████████████████████| 132/132 [04:03<00:00,  1.84s/it]\n",
      "Average Loss = 0.3667): 100%|█████████████████████████████████████████████████████████| 132/132 [04:03<00:00,  1.84s/it]\n",
      "Average Loss = 0.2509): 100%|█████████████████████████████████████████████████████████| 132/132 [04:03<00:00,  1.85s/it]\n",
      "Average Loss = 0.2573): 100%|█████████████████████████████████████████████████████████| 132/132 [04:03<00:00,  1.85s/it]\n",
      "Average Loss = 0.2148): 100%|█████████████████████████████████████████████████████████| 132/132 [04:10<00:00,  1.90s/it]\n",
      "Average Loss = 0.1877): 100%|█████████████████████████████████████████████████████████| 132/132 [04:11<00:00,  1.90s/it]\n",
      "Average Loss = 0.1499): 100%|█████████████████████████████████████████████████████████| 132/132 [04:09<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epoch):\n",
    "    train(model, train_dataloader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45799b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'squad_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13045e",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7d509",
   "metadata": {},
   "source": [
    "학습한 모델을 로딩해서 Inference 하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22d4443f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "#model2 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2)\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels = 2)\n",
    "model2 = BertForQuestionAnswering(bert_config)\n",
    "                                       \n",
    "# 학습한 모델 로딩\n",
    "model2.load_state_dict(torch.load('./squad_model.bin', map_location='cpu'))\n",
    "#model.load_state_dict(torch.load('cola_model_no_pretrained.bin', map_location='cpu'))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1469af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/13/2022 22:37:37 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "08/13/2022 22:37:40 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='squad', config_name='', device=device(type='cpu'), do_eval=False, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_best_size=20, n_gpu=0, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='outputs', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='squad/dev-v2.0.json', save_steps=50, seed=42, server_ip='', server_port='', tokenizer_name='', train_file='squad/train-v2.0.json', verbose_logging=False, version_2_with_negative=True, warmup_steps=0, weight_decay=0.0)\n",
      "08/13/2022 22:37:45 - INFO - __main__ -   Creating features from dataset file at squad/dev-v2.0.json\n",
      "reading squad examples: 100%|██████████████████| 35/35 [00:00<00:00, 147.68it/s]\n",
      "08/13/2022 22:37:45 - INFO - __main__ -   Saving examples into cached file squad/cached_dev_bert-base-uncased_384_features\n",
      "convertting examples to features:   0%|               | 0/11873 [00:00<?, ?it/s]08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000000\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] in what country is normandy located ? [SEP] the norman ##s ( norman : no ##ur ##man ##ds ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10th and 11th centuries gave their name to normandy , a region in france . they were descended from norse ( \" norman \" comes from \" norse ##man \" ) raiders and pirates from denmark , iceland and norway who , under their leader roll ##o , agreed to swear fe ##al ##ty to king charles iii of west fran ##cia . through generations of assimilation and mixing with the native frankish and roman - gaul ##ish populations , their descendants would gradually merge with the carol ##ing ##ian - based cultures of west fran ##cia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10th century , and it continued to evolve over the succeeding centuries . [SEP]\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 9:0 10:1 11:1 12:2 13:2 14:2 15:3 16:3 17:3 18:3 19:3 20:4 21:4 22:5 23:5 24:5 25:6 26:6 27:7 28:7 29:7 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:21 44:22 45:22 46:23 47:24 48:25 49:26 50:26 51:27 52:28 53:29 54:30 55:31 56:32 57:32 58:32 59:32 60:33 61:34 62:35 63:35 64:35 65:35 66:35 67:36 68:37 69:38 70:39 71:40 72:40 73:41 74:42 75:43 76:44 77:44 78:45 79:46 80:47 81:48 82:48 83:48 84:49 85:50 86:51 87:52 88:52 89:52 90:53 91:54 92:55 93:56 94:57 95:58 96:59 97:59 98:59 99:60 100:61 101:62 102:63 103:64 104:65 105:66 106:67 107:68 108:69 109:70 110:71 111:71 112:71 113:71 114:72 115:72 116:73 117:74 118:75 119:76 120:77 121:78 122:79 123:80 124:80 125:80 126:80 127:80 128:81 129:82 130:83 131:84 132:84 133:84 134:85 135:86 136:87 137:88 138:89 139:90 140:91 141:92 142:93 143:93 144:94 145:95 146:96 147:97 148:98 149:99 150:100 151:101 152:102 153:103 154:103 155:104 156:105 157:106 158:107 159:108 160:109 161:110 162:111 163:112 164:112\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 1999 2054 2406 2003 13298 2284 1029 102 1996 5879 2015 1006 5879 1024 2053 3126 2386 5104 1025 2413 1024 5879 5104 1025 3763 1024 5879 3490 1007 2020 1996 2111 2040 1999 1996 6049 1998 6252 4693 2435 2037 2171 2000 13298 1010 1037 2555 1999 2605 1012 2027 2020 9287 2013 15342 1006 1000 5879 1000 3310 2013 1000 15342 2386 1000 1007 10642 1998 8350 2013 5842 1010 10399 1998 5120 2040 1010 2104 2037 3003 4897 2080 1010 3530 2000 8415 10768 2389 3723 2000 2332 2798 3523 1997 2225 23151 7405 1012 2083 8213 1997 27574 1998 6809 2007 1996 3128 26165 1998 3142 1011 26522 4509 7080 1010 2037 8481 2052 6360 13590 2007 1996 8594 2075 2937 1011 2241 8578 1997 2225 23151 7405 1012 1996 5664 3451 1998 5636 4767 1997 1996 5879 2015 6003 3322 1999 1996 2034 2431 1997 1996 6049 2301 1010 1998 2009 2506 2000 19852 2058 1996 13034 4693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000001\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 1\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] when were the norman ##s in normandy ? [SEP] the norman ##s ( norman : no ##ur ##man ##ds ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10th and 11th centuries gave their name to normandy , a region in france . they were descended from norse ( \" norman \" comes from \" norse ##man \" ) raiders and pirates from denmark , iceland and norway who , under their leader roll ##o , agreed to swear fe ##al ##ty to king charles iii of west fran ##cia . through generations of assimilation and mixing with the native frankish and roman - gaul ##ish populations , their descendants would gradually merge with the carol ##ing ##ian - based cultures of west fran ##cia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10th century , and it continued to evolve over the succeeding centuries . [SEP]\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 10:0 11:1 12:1 13:2 14:2 15:2 16:3 17:3 18:3 19:3 20:3 21:4 22:4 23:5 24:5 25:5 26:6 27:6 28:7 29:7 30:7 31:8 32:9 33:10 34:11 35:12 36:13 37:14 38:15 39:16 40:17 41:18 42:19 43:20 44:21 45:22 46:22 47:23 48:24 49:25 50:26 51:26 52:27 53:28 54:29 55:30 56:31 57:32 58:32 59:32 60:32 61:33 62:34 63:35 64:35 65:35 66:35 67:35 68:36 69:37 70:38 71:39 72:40 73:40 74:41 75:42 76:43 77:44 78:44 79:45 80:46 81:47 82:48 83:48 84:48 85:49 86:50 87:51 88:52 89:52 90:52 91:53 92:54 93:55 94:56 95:57 96:58 97:59 98:59 99:59 100:60 101:61 102:62 103:63 104:64 105:65 106:66 107:67 108:68 109:69 110:70 111:71 112:71 113:71 114:71 115:72 116:72 117:73 118:74 119:75 120:76 121:77 122:78 123:79 124:80 125:80 126:80 127:80 128:80 129:81 130:82 131:83 132:84 133:84 134:84 135:85 136:86 137:87 138:88 139:89 140:90 141:91 142:92 143:93 144:93 145:94 146:95 147:96 148:97 149:98 150:99 151:100 152:101 153:102 154:103 155:103 156:104 157:105 158:106 159:107 160:108 161:109 162:110 163:111 164:112 165:112\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2043 2020 1996 5879 2015 1999 13298 1029 102 1996 5879 2015 1006 5879 1024 2053 3126 2386 5104 1025 2413 1024 5879 5104 1025 3763 1024 5879 3490 1007 2020 1996 2111 2040 1999 1996 6049 1998 6252 4693 2435 2037 2171 2000 13298 1010 1037 2555 1999 2605 1012 2027 2020 9287 2013 15342 1006 1000 5879 1000 3310 2013 1000 15342 2386 1000 1007 10642 1998 8350 2013 5842 1010 10399 1998 5120 2040 1010 2104 2037 3003 4897 2080 1010 3530 2000 8415 10768 2389 3723 2000 2332 2798 3523 1997 2225 23151 7405 1012 2083 8213 1997 27574 1998 6809 2007 1996 3128 26165 1998 3142 1011 26522 4509 7080 1010 2037 8481 2052 6360 13590 2007 1996 8594 2075 2937 1011 2241 8578 1997 2225 23151 7405 1012 1996 5664 3451 1998 5636 4767 1997 1996 5879 2015 6003 3322 1999 1996 2034 2431 1997 1996 6049 2301 1010 1998 2009 2506 2000 19852 2058 1996 13034 4693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000002\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 2\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] from which countries did the norse originate ? [SEP] the norman ##s ( norman : no ##ur ##man ##ds ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10th and 11th centuries gave their name to normandy , a region in france . they were descended from norse ( \" norman \" comes from \" norse ##man \" ) raiders and pirates from denmark , iceland and norway who , under their leader roll ##o , agreed to swear fe ##al ##ty to king charles iii of west fran ##cia . through generations of assimilation and mixing with the native frankish and roman - gaul ##ish populations , their descendants would gradually merge with the carol ##ing ##ian - based cultures of west fran ##cia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10th century , and it continued to evolve over the succeeding centuries . [SEP]\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 10:0 11:1 12:1 13:2 14:2 15:2 16:3 17:3 18:3 19:3 20:3 21:4 22:4 23:5 24:5 25:5 26:6 27:6 28:7 29:7 30:7 31:8 32:9 33:10 34:11 35:12 36:13 37:14 38:15 39:16 40:17 41:18 42:19 43:20 44:21 45:22 46:22 47:23 48:24 49:25 50:26 51:26 52:27 53:28 54:29 55:30 56:31 57:32 58:32 59:32 60:32 61:33 62:34 63:35 64:35 65:35 66:35 67:35 68:36 69:37 70:38 71:39 72:40 73:40 74:41 75:42 76:43 77:44 78:44 79:45 80:46 81:47 82:48 83:48 84:48 85:49 86:50 87:51 88:52 89:52 90:52 91:53 92:54 93:55 94:56 95:57 96:58 97:59 98:59 99:59 100:60 101:61 102:62 103:63 104:64 105:65 106:66 107:67 108:68 109:69 110:70 111:71 112:71 113:71 114:71 115:72 116:72 117:73 118:74 119:75 120:76 121:77 122:78 123:79 124:80 125:80 126:80 127:80 128:80 129:81 130:82 131:83 132:84 133:84 134:84 135:85 136:86 137:87 138:88 139:89 140:90 141:91 142:92 143:93 144:93 145:94 146:95 147:96 148:97 149:98 150:99 151:100 152:101 153:102 154:103 155:103 156:104 157:105 158:106 159:107 160:108 161:109 162:110 163:111 164:112 165:112\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2013 2029 3032 2106 1996 15342 21754 1029 102 1996 5879 2015 1006 5879 1024 2053 3126 2386 5104 1025 2413 1024 5879 5104 1025 3763 1024 5879 3490 1007 2020 1996 2111 2040 1999 1996 6049 1998 6252 4693 2435 2037 2171 2000 13298 1010 1037 2555 1999 2605 1012 2027 2020 9287 2013 15342 1006 1000 5879 1000 3310 2013 1000 15342 2386 1000 1007 10642 1998 8350 2013 5842 1010 10399 1998 5120 2040 1010 2104 2037 3003 4897 2080 1010 3530 2000 8415 10768 2389 3723 2000 2332 2798 3523 1997 2225 23151 7405 1012 2083 8213 1997 27574 1998 6809 2007 1996 3128 26165 1998 3142 1011 26522 4509 7080 1010 2037 8481 2052 6360 13590 2007 1996 8594 2075 2937 1011 2241 8578 1997 2225 23151 7405 1012 1996 5664 3451 1998 5636 4767 1997 1996 5879 2015 6003 3322 1999 1996 2034 2431 1997 1996 6049 2301 1010 1998 2009 2506 2000 19852 2058 1996 13034 4693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000003\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 3\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] who was the norse leader ? [SEP] the norman ##s ( norman : no ##ur ##man ##ds ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10th and 11th centuries gave their name to normandy , a region in france . they were descended from norse ( \" norman \" comes from \" norse ##man \" ) raiders and pirates from denmark , iceland and norway who , under their leader roll ##o , agreed to swear fe ##al ##ty to king charles iii of west fran ##cia . through generations of assimilation and mixing with the native frankish and roman - gaul ##ish populations , their descendants would gradually merge with the carol ##ing ##ian - based cultures of west fran ##cia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10th century , and it continued to evolve over the succeeding centuries . [SEP]\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 8:0 9:1 10:1 11:2 12:2 13:2 14:3 15:3 16:3 17:3 18:3 19:4 20:4 21:5 22:5 23:5 24:6 25:6 26:7 27:7 28:7 29:8 30:9 31:10 32:11 33:12 34:13 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:21 43:22 44:22 45:23 46:24 47:25 48:26 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:32 57:32 58:32 59:33 60:34 61:35 62:35 63:35 64:35 65:35 66:36 67:37 68:38 69:39 70:40 71:40 72:41 73:42 74:43 75:44 76:44 77:45 78:46 79:47 80:48 81:48 82:48 83:49 84:50 85:51 86:52 87:52 88:52 89:53 90:54 91:55 92:56 93:57 94:58 95:59 96:59 97:59 98:60 99:61 100:62 101:63 102:64 103:65 104:66 105:67 106:68 107:69 108:70 109:71 110:71 111:71 112:71 113:72 114:72 115:73 116:74 117:75 118:76 119:77 120:78 121:79 122:80 123:80 124:80 125:80 126:80 127:81 128:82 129:83 130:84 131:84 132:84 133:85 134:86 135:87 136:88 137:89 138:90 139:91 140:92 141:93 142:93 143:94 144:95 145:96 146:97 147:98 148:99 149:100 150:101 151:102 152:103 153:103 154:104 155:105 156:106 157:107 158:108 159:109 160:110 161:111 162:112 163:112\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2040 2001 1996 15342 3003 1029 102 1996 5879 2015 1006 5879 1024 2053 3126 2386 5104 1025 2413 1024 5879 5104 1025 3763 1024 5879 3490 1007 2020 1996 2111 2040 1999 1996 6049 1998 6252 4693 2435 2037 2171 2000 13298 1010 1037 2555 1999 2605 1012 2027 2020 9287 2013 15342 1006 1000 5879 1000 3310 2013 1000 15342 2386 1000 1007 10642 1998 8350 2013 5842 1010 10399 1998 5120 2040 1010 2104 2037 3003 4897 2080 1010 3530 2000 8415 10768 2389 3723 2000 2332 2798 3523 1997 2225 23151 7405 1012 2083 8213 1997 27574 1998 6809 2007 1996 3128 26165 1998 3142 1011 26522 4509 7080 1010 2037 8481 2052 6360 13590 2007 1996 8594 2075 2937 1011 2241 8578 1997 2225 23151 7405 1012 1996 5664 3451 1998 5636 4767 1997 1996 5879 2015 6003 3322 1999 1996 2034 2431 1997 1996 6049 2301 1010 1998 2009 2506 2000 19852 2058 1996 13034 4693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000004\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 4\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] what century did the norman ##s first gain their separate identity ? [SEP] the norman ##s ( norman : no ##ur ##man ##ds ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10th and 11th centuries gave their name to normandy , a region in france . they were descended from norse ( \" norman \" comes from \" norse ##man \" ) raiders and pirates from denmark , iceland and norway who , under their leader roll ##o , agreed to swear fe ##al ##ty to king charles iii of west fran ##cia . through generations of assimilation and mixing with the native frankish and roman - gaul ##ish populations , their descendants would gradually merge with the carol ##ing ##ian - based cultures of west fran ##cia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10th century , and it continued to evolve over the succeeding centuries . [SEP]\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 14:0 15:1 16:1 17:2 18:2 19:2 20:3 21:3 22:3 23:3 24:3 25:4 26:4 27:5 28:5 29:5 30:6 31:6 32:7 33:7 34:7 35:8 36:9 37:10 38:11 39:12 40:13 41:14 42:15 43:16 44:17 45:18 46:19 47:20 48:21 49:22 50:22 51:23 52:24 53:25 54:26 55:26 56:27 57:28 58:29 59:30 60:31 61:32 62:32 63:32 64:32 65:33 66:34 67:35 68:35 69:35 70:35 71:35 72:36 73:37 74:38 75:39 76:40 77:40 78:41 79:42 80:43 81:44 82:44 83:45 84:46 85:47 86:48 87:48 88:48 89:49 90:50 91:51 92:52 93:52 94:52 95:53 96:54 97:55 98:56 99:57 100:58 101:59 102:59 103:59 104:60 105:61 106:62 107:63 108:64 109:65 110:66 111:67 112:68 113:69 114:70 115:71 116:71 117:71 118:71 119:72 120:72 121:73 122:74 123:75 124:76 125:77 126:78 127:79 128:80 129:80 130:80 131:80 132:80 133:81 134:82 135:83 136:84 137:84 138:84 139:85 140:86 141:87 142:88 143:89 144:90 145:91 146:92 147:93 148:93 149:94 150:95 151:96 152:97 153:98 154:99 155:100 156:101 157:102 158:103 159:103 160:104 161:105 162:106 163:107 164:108 165:109 166:110 167:111 168:112 169:112\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2054 2301 2106 1996 5879 2015 2034 5114 2037 3584 4767 1029 102 1996 5879 2015 1006 5879 1024 2053 3126 2386 5104 1025 2413 1024 5879 5104 1025 3763 1024 5879 3490 1007 2020 1996 2111 2040 1999 1996 6049 1998 6252 4693 2435 2037 2171 2000 13298 1010 1037 2555 1999 2605 1012 2027 2020 9287 2013 15342 1006 1000 5879 1000 3310 2013 1000 15342 2386 1000 1007 10642 1998 8350 2013 5842 1010 10399 1998 5120 2040 1010 2104 2037 3003 4897 2080 1010 3530 2000 8415 10768 2389 3723 2000 2332 2798 3523 1997 2225 23151 7405 1012 2083 8213 1997 27574 1998 6809 2007 1996 3128 26165 1998 3142 1011 26522 4509 7080 1010 2037 8481 2052 6360 13590 2007 1996 8594 2075 2937 1011 2241 8578 1997 2225 23151 7405 1012 1996 5664 3451 1998 5636 4767 1997 1996 5879 2015 6003 3322 1999 1996 2034 2431 1997 1996 6049 2301 1010 1998 2009 2506 2000 19852 2058 1996 13034 4693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000005\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 5\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] who gave their name to normandy in the 1000 ' s and 1100 ' s [SEP] the norman ##s ( norman : no ##ur ##man ##ds ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10th and 11th centuries gave their name to normandy , a region in france . they were descended from norse ( \" norman \" comes from \" norse ##man \" ) raiders and pirates from denmark , iceland and norway who , under their leader roll ##o , agreed to swear fe ##al ##ty to king charles iii of west fran ##cia . through generations of assimilation and mixing with the native frankish and roman - gaul ##ish populations , their descendants would gradually merge with the carol ##ing ##ian - based cultures of west fran ##cia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10th century , and it continued to evolve over the succeeding centuries . [SEP]\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 17:0 18:1 19:1 20:2 21:2 22:2 23:3 24:3 25:3 26:3 27:3 28:4 29:4 30:5 31:5 32:5 33:6 34:6 35:7 36:7 37:7 38:8 39:9 40:10 41:11 42:12 43:13 44:14 45:15 46:16 47:17 48:18 49:19 50:20 51:21 52:22 53:22 54:23 55:24 56:25 57:26 58:26 59:27 60:28 61:29 62:30 63:31 64:32 65:32 66:32 67:32 68:33 69:34 70:35 71:35 72:35 73:35 74:35 75:36 76:37 77:38 78:39 79:40 80:40 81:41 82:42 83:43 84:44 85:44 86:45 87:46 88:47 89:48 90:48 91:48 92:49 93:50 94:51 95:52 96:52 97:52 98:53 99:54 100:55 101:56 102:57 103:58 104:59 105:59 106:59 107:60 108:61 109:62 110:63 111:64 112:65 113:66 114:67 115:68 116:69 117:70 118:71 119:71 120:71 121:71 122:72 123:72 124:73 125:74 126:75 127:76 128:77 129:78 130:79 131:80 132:80 133:80 134:80 135:80 136:81 137:82 138:83 139:84 140:84 141:84 142:85 143:86 144:87 145:88 146:89 147:90 148:91 149:92 150:93 151:93 152:94 153:95 154:96 155:97 156:98 157:99 158:100 159:101 160:102 161:103 162:103 163:104 164:105 165:106 166:107 167:108 168:109 169:110 170:111 171:112 172:112\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2040 2435 2037 2171 2000 13298 1999 1996 6694 1005 1055 1998 22096 1005 1055 102 1996 5879 2015 1006 5879 1024 2053 3126 2386 5104 1025 2413 1024 5879 5104 1025 3763 1024 5879 3490 1007 2020 1996 2111 2040 1999 1996 6049 1998 6252 4693 2435 2037 2171 2000 13298 1010 1037 2555 1999 2605 1012 2027 2020 9287 2013 15342 1006 1000 5879 1000 3310 2013 1000 15342 2386 1000 1007 10642 1998 8350 2013 5842 1010 10399 1998 5120 2040 1010 2104 2037 3003 4897 2080 1010 3530 2000 8415 10768 2389 3723 2000 2332 2798 3523 1997 2225 23151 7405 1012 2083 8213 1997 27574 1998 6809 2007 1996 3128 26165 1998 3142 1011 26522 4509 7080 1010 2037 8481 2052 6360 13590 2007 1996 8594 2075 2937 1011 2241 8578 1997 2225 23151 7405 1012 1996 5664 3451 1998 5636 4767 1997 1996 5879 2015 6003 3322 1999 1996 2034 2431 1997 1996 6049 2301 1010 1998 2009 2506 2000 19852 2058 1996 13034 4693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000006\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 6\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] what is france a region of ? [SEP] the norman ##s ( norman : no ##ur ##man ##ds ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10th and 11th centuries gave their name to normandy , a region in france . they were descended from norse ( \" norman \" comes from \" norse ##man \" ) raiders and pirates from denmark , iceland and norway who , under their leader roll ##o , agreed to swear fe ##al ##ty to king charles iii of west fran ##cia . through generations of assimilation and mixing with the native frankish and roman - gaul ##ish populations , their descendants would gradually merge with the carol ##ing ##ian - based cultures of west fran ##cia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10th century , and it continued to evolve over the succeeding centuries . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 9:0 10:1 11:1 12:2 13:2 14:2 15:3 16:3 17:3 18:3 19:3 20:4 21:4 22:5 23:5 24:5 25:6 26:6 27:7 28:7 29:7 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:21 44:22 45:22 46:23 47:24 48:25 49:26 50:26 51:27 52:28 53:29 54:30 55:31 56:32 57:32 58:32 59:32 60:33 61:34 62:35 63:35 64:35 65:35 66:35 67:36 68:37 69:38 70:39 71:40 72:40 73:41 74:42 75:43 76:44 77:44 78:45 79:46 80:47 81:48 82:48 83:48 84:49 85:50 86:51 87:52 88:52 89:52 90:53 91:54 92:55 93:56 94:57 95:58 96:59 97:59 98:59 99:60 100:61 101:62 102:63 103:64 104:65 105:66 106:67 107:68 108:69 109:70 110:71 111:71 112:71 113:71 114:72 115:72 116:73 117:74 118:75 119:76 120:77 121:78 122:79 123:80 124:80 125:80 126:80 127:80 128:81 129:82 130:83 131:84 132:84 133:84 134:85 135:86 136:87 137:88 138:89 139:90 140:91 141:92 142:93 143:93 144:94 145:95 146:96 147:97 148:98 149:99 150:100 151:101 152:102 153:103 154:103 155:104 156:105 157:106 158:107 159:108 160:109 161:110 162:111 163:112 164:112\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2054 2003 2605 1037 2555 1997 1029 102 1996 5879 2015 1006 5879 1024 2053 3126 2386 5104 1025 2413 1024 5879 5104 1025 3763 1024 5879 3490 1007 2020 1996 2111 2040 1999 1996 6049 1998 6252 4693 2435 2037 2171 2000 13298 1010 1037 2555 1999 2605 1012 2027 2020 9287 2013 15342 1006 1000 5879 1000 3310 2013 1000 15342 2386 1000 1007 10642 1998 8350 2013 5842 1010 10399 1998 5120 2040 1010 2104 2037 3003 4897 2080 1010 3530 2000 8415 10768 2389 3723 2000 2332 2798 3523 1997 2225 23151 7405 1012 2083 8213 1997 27574 1998 6809 2007 1996 3128 26165 1998 3142 1011 26522 4509 7080 1010 2037 8481 2052 6360 13590 2007 1996 8594 2075 2937 1011 2241 8578 1997 2225 23151 7405 1012 1996 5664 3451 1998 5636 4767 1997 1996 5879 2015 6003 3322 1999 1996 2034 2431 1997 1996 6049 2301 1010 1998 2009 2506 2000 19852 2058 1996 13034 4693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000007\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 7\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] who did king charles iii swear fe ##al ##ty to ? [SEP] the norman ##s ( norman : no ##ur ##man ##ds ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10th and 11th centuries gave their name to normandy , a region in france . they were descended from norse ( \" norman \" comes from \" norse ##man \" ) raiders and pirates from denmark , iceland and norway who , under their leader roll ##o , agreed to swear fe ##al ##ty to king charles iii of west fran ##cia . through generations of assimilation and mixing with the native frankish and roman - gaul ##ish populations , their descendants would gradually merge with the carol ##ing ##ian - based cultures of west fran ##cia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10th century , and it continued to evolve over the succeeding centuries . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 13:0 14:1 15:1 16:2 17:2 18:2 19:3 20:3 21:3 22:3 23:3 24:4 25:4 26:5 27:5 28:5 29:6 30:6 31:7 32:7 33:7 34:8 35:9 36:10 37:11 38:12 39:13 40:14 41:15 42:16 43:17 44:18 45:19 46:20 47:21 48:22 49:22 50:23 51:24 52:25 53:26 54:26 55:27 56:28 57:29 58:30 59:31 60:32 61:32 62:32 63:32 64:33 65:34 66:35 67:35 68:35 69:35 70:35 71:36 72:37 73:38 74:39 75:40 76:40 77:41 78:42 79:43 80:44 81:44 82:45 83:46 84:47 85:48 86:48 87:48 88:49 89:50 90:51 91:52 92:52 93:52 94:53 95:54 96:55 97:56 98:57 99:58 100:59 101:59 102:59 103:60 104:61 105:62 106:63 107:64 108:65 109:66 110:67 111:68 112:69 113:70 114:71 115:71 116:71 117:71 118:72 119:72 120:73 121:74 122:75 123:76 124:77 125:78 126:79 127:80 128:80 129:80 130:80 131:80 132:81 133:82 134:83 135:84 136:84 137:84 138:85 139:86 140:87 141:88 142:89 143:90 144:91 145:92 146:93 147:93 148:94 149:95 150:96 151:97 152:98 153:99 154:100 155:101 156:102 157:103 158:103 159:104 160:105 161:106 162:107 163:108 164:109 165:110 166:111 167:112 168:112\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2040 2106 2332 2798 3523 8415 10768 2389 3723 2000 1029 102 1996 5879 2015 1006 5879 1024 2053 3126 2386 5104 1025 2413 1024 5879 5104 1025 3763 1024 5879 3490 1007 2020 1996 2111 2040 1999 1996 6049 1998 6252 4693 2435 2037 2171 2000 13298 1010 1037 2555 1999 2605 1012 2027 2020 9287 2013 15342 1006 1000 5879 1000 3310 2013 1000 15342 2386 1000 1007 10642 1998 8350 2013 5842 1010 10399 1998 5120 2040 1010 2104 2037 3003 4897 2080 1010 3530 2000 8415 10768 2389 3723 2000 2332 2798 3523 1997 2225 23151 7405 1012 2083 8213 1997 27574 1998 6809 2007 1996 3128 26165 1998 3142 1011 26522 4509 7080 1010 2037 8481 2052 6360 13590 2007 1996 8594 2075 2937 1011 2241 8578 1997 2225 23151 7405 1012 1996 5664 3451 1998 5636 4767 1997 1996 5879 2015 6003 3322 1999 1996 2034 2431 1997 1996 6049 2301 1010 1998 2009 2506 2000 19852 2058 1996 13034 4693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000008\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 8\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] when did the frankish identity emerge ? [SEP] the norman ##s ( norman : no ##ur ##man ##ds ; french : norman ##ds ; latin : norman ##ni ) were the people who in the 10th and 11th centuries gave their name to normandy , a region in france . they were descended from norse ( \" norman \" comes from \" norse ##man \" ) raiders and pirates from denmark , iceland and norway who , under their leader roll ##o , agreed to swear fe ##al ##ty to king charles iii of west fran ##cia . through generations of assimilation and mixing with the native frankish and roman - gaul ##ish populations , their descendants would gradually merge with the carol ##ing ##ian - based cultures of west fran ##cia . the distinct cultural and ethnic identity of the norman ##s emerged initially in the first half of the 10th century , and it continued to evolve over the succeeding centuries . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 9:0 10:1 11:1 12:2 13:2 14:2 15:3 16:3 17:3 18:3 19:3 20:4 21:4 22:5 23:5 24:5 25:6 26:6 27:7 28:7 29:7 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:21 44:22 45:22 46:23 47:24 48:25 49:26 50:26 51:27 52:28 53:29 54:30 55:31 56:32 57:32 58:32 59:32 60:33 61:34 62:35 63:35 64:35 65:35 66:35 67:36 68:37 69:38 70:39 71:40 72:40 73:41 74:42 75:43 76:44 77:44 78:45 79:46 80:47 81:48 82:48 83:48 84:49 85:50 86:51 87:52 88:52 89:52 90:53 91:54 92:55 93:56 94:57 95:58 96:59 97:59 98:59 99:60 100:61 101:62 102:63 103:64 104:65 105:66 106:67 107:68 108:69 109:70 110:71 111:71 112:71 113:71 114:72 115:72 116:73 117:74 118:75 119:76 120:77 121:78 122:79 123:80 124:80 125:80 126:80 127:80 128:81 129:82 130:83 131:84 132:84 133:84 134:85 135:86 136:87 137:88 138:89 139:90 140:91 141:92 142:93 143:93 144:94 145:95 146:96 147:97 148:98 149:99 150:100 151:101 152:102 153:103 154:103 155:104 156:105 157:106 158:107 159:108 160:109 161:110 162:111 163:112 164:112\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2043 2106 1996 26165 4767 12636 1029 102 1996 5879 2015 1006 5879 1024 2053 3126 2386 5104 1025 2413 1024 5879 5104 1025 3763 1024 5879 3490 1007 2020 1996 2111 2040 1999 1996 6049 1998 6252 4693 2435 2037 2171 2000 13298 1010 1037 2555 1999 2605 1012 2027 2020 9287 2013 15342 1006 1000 5879 1000 3310 2013 1000 15342 2386 1000 1007 10642 1998 8350 2013 5842 1010 10399 1998 5120 2040 1010 2104 2037 3003 4897 2080 1010 3530 2000 8415 10768 2389 3723 2000 2332 2798 3523 1997 2225 23151 7405 1012 2083 8213 1997 27574 1998 6809 2007 1996 3128 26165 1998 3142 1011 26522 4509 7080 1010 2037 8481 2052 6360 13590 2007 1996 8594 2075 2937 1011 2241 8578 1997 2225 23151 7405 1012 1996 5664 3451 1998 5636 4767 1997 1996 5879 2015 6003 3322 1999 1996 2034 2431 1997 1996 6049 2301 1010 1998 2009 2506 2000 19852 2058 1996 13034 4693 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000009\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 9\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] who was the duke in the battle of hastings ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were famed for their martial spirit and eventually for their christian pie ##ty , becoming expo ##nent ##s of the catholic orthodoxy into which they ass ##imi ##lated . they adopted the gallo - romance language of the frankish land they settled , their dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a co ##hesive and formidable principality in feudal tenure . the norman ##s are noted both for their culture , such as their unique romanesque architecture and musical traditions , and for their significant military accomplishments and innovations . norman adventurer ##s founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the sara ##cens and byzantine ##s , and an expedition on behalf of their duke , william the conqueror , led to the norman conquest of england at the battle of hastings in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where their prince bo ##hem ##ond i founded the principality of antioch in the levant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:6 20:7 21:8 22:9 23:10 24:11 25:12 26:13 27:14 28:15 29:16 30:17 31:18 32:18 33:19 34:20 35:20 36:21 37:22 38:23 39:24 40:25 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:32 49:32 50:33 51:34 52:34 53:34 54:35 55:36 56:37 57:38 58:39 59:40 60:41 61:42 62:42 63:42 64:42 65:43 66:44 67:45 68:46 69:46 70:46 71:47 72:48 73:49 74:50 75:51 76:52 77:53 78:53 79:54 80:55 81:56 82:57 83:58 84:59 85:59 86:60 87:60 88:61 89:62 90:63 91:63 92:64 93:65 94:66 95:67 96:67 97:68 98:69 99:70 100:71 101:71 102:72 103:73 104:74 105:75 106:76 107:77 108:78 109:79 110:80 111:80 112:81 113:82 114:83 115:84 116:84 117:85 118:86 119:87 120:87 121:88 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:98 133:99 134:100 135:101 136:102 137:103 138:104 139:104 140:105 141:106 142:106 143:107 144:108 145:109 146:110 147:111 148:112 149:112 150:113 151:114 152:115 153:116 154:117 155:118 156:119 157:120 158:121 159:121 160:122 161:123 162:124 163:125 164:126 165:127 166:128 167:129 168:129 169:130 170:131 171:131 172:132 173:133 174:134 175:135 176:136 177:137 178:138 179:139 180:140 181:141 182:141 183:142 184:143 185:144 186:145 187:146 188:146 189:147 190:148 191:148 192:148 193:149 194:150 195:151 196:152 197:153 198:154 199:155 200:156 201:156 202:157 203:158 204:159 205:159 206:160 207:161 208:162 209:163 210:164 211:165 212:166 213:167 214:168 215:169 216:170 217:171 218:172 219:173 220:173 221:173 222:174 223:175 224:176 225:177 226:178 227:179 228:180 229:181 230:182 231:183 232:184 233:185 234:186 235:187 236:188 237:189 238:190 239:191 240:192 241:192 242:193 243:194 244:195 245:196 246:196 247:196 248:197 249:198 250:199 251:200 252:201 253:202 254:203 255:204 256:205 257:205 258:206 259:207 260:208 261:209 262:210 263:211 264:212 265:212 266:213 267:214 268:214 269:215 270:216 271:217 272:218 273:219 274:220 275:221 276:222 277:223 278:224 279:225 280:225\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2040 2001 1996 3804 1999 1996 2645 1997 12296 1029 102 1996 5879 5321 2018 1037 2350 2576 1010 3451 1998 2510 4254 2006 5781 2885 1998 2130 1996 2379 2264 1012 1996 5879 2015 2020 15607 2005 2037 7761 4382 1998 2776 2005 2037 3017 11345 3723 1010 3352 16258 21576 2015 1997 1996 3234 26582 2046 2029 2027 4632 27605 13776 1012 2027 4233 1996 25624 1011 7472 2653 1997 1996 26165 2455 2027 3876 1010 2037 9329 3352 2124 2004 5879 1010 20692 8630 2030 5879 2413 1010 2019 2590 4706 2653 1012 1996 11068 1997 13298 1010 2029 2027 2719 2011 5036 2007 1996 2413 4410 1010 2001 1037 2307 10882 12879 1997 5781 2605 1010 1998 2104 2957 1045 1997 13298 2001 16158 2046 1037 2522 21579 1998 18085 18018 1999 16708 7470 1012 1996 5879 2015 2024 3264 2119 2005 2037 3226 1010 2107 2004 2037 4310 17135 4294 1998 3315 7443 1010 1998 2005 2037 3278 2510 17571 1998 15463 1012 5879 29506 2015 2631 1996 2983 1997 12071 2104 5074 2462 2044 16152 2075 2670 3304 2006 1996 7354 19023 1998 8734 2015 1010 1998 2019 5590 2006 6852 1997 2037 3804 1010 2520 1996 25466 1010 2419 2000 1996 5879 9187 1997 2563 2012 1996 2645 1997 12296 1999 10114 2575 1012 5879 3451 1998 2510 3747 3659 2013 2122 2047 2647 8941 2000 1996 25237 2163 1997 1996 2379 2264 1010 2073 2037 3159 8945 29122 15422 1045 2631 1996 18018 1997 19078 1999 1996 24485 1010 2000 3885 1998 3575 1999 2307 3725 1010 2000 3163 1010 1998 2000 1996 20266 1997 2167 3088 1998 1996 17154 3470 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000010\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 10\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] who ruled the duchy of normandy [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were famed for their martial spirit and eventually for their christian pie ##ty , becoming expo ##nent ##s of the catholic orthodoxy into which they ass ##imi ##lated . they adopted the gallo - romance language of the frankish land they settled , their dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a co ##hesive and formidable principality in feudal tenure . the norman ##s are noted both for their culture , such as their unique romanesque architecture and musical traditions , and for their significant military accomplishments and innovations . norman adventurer ##s founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the sara ##cens and byzantine ##s , and an expedition on behalf of their duke , william the conqueror , led to the norman conquest of england at the battle of hastings in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where their prince bo ##hem ##ond i founded the principality of antioch in the levant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:13 23:14 24:15 25:16 26:17 27:18 28:18 29:19 30:20 31:20 32:21 33:22 34:23 35:24 36:25 37:26 38:27 39:28 40:29 41:30 42:31 43:32 44:32 45:32 46:33 47:34 48:34 49:34 50:35 51:36 52:37 53:38 54:39 55:40 56:41 57:42 58:42 59:42 60:42 61:43 62:44 63:45 64:46 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:55 77:56 78:57 79:58 80:59 81:59 82:60 83:60 84:61 85:62 86:63 87:63 88:64 89:65 90:66 91:67 92:67 93:68 94:69 95:70 96:71 97:71 98:72 99:73 100:74 101:75 102:76 103:77 104:78 105:79 106:80 107:80 108:81 109:82 110:83 111:84 112:84 113:85 114:86 115:87 116:87 117:88 118:89 119:90 120:91 121:92 122:93 123:94 124:95 125:96 126:97 127:98 128:98 129:99 130:100 131:101 132:102 133:103 134:104 135:104 136:105 137:106 138:106 139:107 140:108 141:109 142:110 143:111 144:112 145:112 146:113 147:114 148:115 149:116 150:117 151:118 152:119 153:120 154:121 155:121 156:122 157:123 158:124 159:125 160:126 161:127 162:128 163:129 164:129 165:130 166:131 167:131 168:132 169:133 170:134 171:135 172:136 173:137 174:138 175:139 176:140 177:141 178:141 179:142 180:143 181:144 182:145 183:146 184:146 185:147 186:148 187:148 188:148 189:149 190:150 191:151 192:152 193:153 194:154 195:155 196:156 197:156 198:157 199:158 200:159 201:159 202:160 203:161 204:162 205:163 206:164 207:165 208:166 209:167 210:168 211:169 212:170 213:171 214:172 215:173 216:173 217:173 218:174 219:175 220:176 221:177 222:178 223:179 224:180 225:181 226:182 227:183 228:184 229:185 230:186 231:187 232:188 233:189 234:190 235:191 236:192 237:192 238:193 239:194 240:195 241:196 242:196 243:196 244:197 245:198 246:199 247:200 248:201 249:202 250:203 251:204 252:205 253:205 254:206 255:207 256:208 257:209 258:210 259:211 260:212 261:212 262:213 263:214 264:214 265:215 266:216 267:217 268:218 269:219 270:220 271:221 272:222 273:223 274:224 275:225 276:225\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2040 5451 1996 11068 1997 13298 102 1996 5879 5321 2018 1037 2350 2576 1010 3451 1998 2510 4254 2006 5781 2885 1998 2130 1996 2379 2264 1012 1996 5879 2015 2020 15607 2005 2037 7761 4382 1998 2776 2005 2037 3017 11345 3723 1010 3352 16258 21576 2015 1997 1996 3234 26582 2046 2029 2027 4632 27605 13776 1012 2027 4233 1996 25624 1011 7472 2653 1997 1996 26165 2455 2027 3876 1010 2037 9329 3352 2124 2004 5879 1010 20692 8630 2030 5879 2413 1010 2019 2590 4706 2653 1012 1996 11068 1997 13298 1010 2029 2027 2719 2011 5036 2007 1996 2413 4410 1010 2001 1037 2307 10882 12879 1997 5781 2605 1010 1998 2104 2957 1045 1997 13298 2001 16158 2046 1037 2522 21579 1998 18085 18018 1999 16708 7470 1012 1996 5879 2015 2024 3264 2119 2005 2037 3226 1010 2107 2004 2037 4310 17135 4294 1998 3315 7443 1010 1998 2005 2037 3278 2510 17571 1998 15463 1012 5879 29506 2015 2631 1996 2983 1997 12071 2104 5074 2462 2044 16152 2075 2670 3304 2006 1996 7354 19023 1998 8734 2015 1010 1998 2019 5590 2006 6852 1997 2037 3804 1010 2520 1996 25466 1010 2419 2000 1996 5879 9187 1997 2563 2012 1996 2645 1997 12296 1999 10114 2575 1012 5879 3451 1998 2510 3747 3659 2013 2122 2047 2647 8941 2000 1996 25237 2163 1997 1996 2379 2264 1010 2073 2037 3159 8945 29122 15422 1045 2631 1996 18018 1997 19078 1999 1996 24485 1010 2000 3885 1998 3575 1999 2307 3725 1010 2000 3163 1010 1998 2000 1996 20266 1997 2167 3088 1998 1996 17154 3470 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000011\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 11\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] what religion were the norman ##s [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were famed for their martial spirit and eventually for their christian pie ##ty , becoming expo ##nent ##s of the catholic orthodoxy into which they ass ##imi ##lated . they adopted the gallo - romance language of the frankish land they settled , their dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a co ##hesive and formidable principality in feudal tenure . the norman ##s are noted both for their culture , such as their unique romanesque architecture and musical traditions , and for their significant military accomplishments and innovations . norman adventurer ##s founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the sara ##cens and byzantine ##s , and an expedition on behalf of their duke , william the conqueror , led to the norman conquest of england at the battle of hastings in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where their prince bo ##hem ##ond i founded the principality of antioch in the levant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:13 23:14 24:15 25:16 26:17 27:18 28:18 29:19 30:20 31:20 32:21 33:22 34:23 35:24 36:25 37:26 38:27 39:28 40:29 41:30 42:31 43:32 44:32 45:32 46:33 47:34 48:34 49:34 50:35 51:36 52:37 53:38 54:39 55:40 56:41 57:42 58:42 59:42 60:42 61:43 62:44 63:45 64:46 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:55 77:56 78:57 79:58 80:59 81:59 82:60 83:60 84:61 85:62 86:63 87:63 88:64 89:65 90:66 91:67 92:67 93:68 94:69 95:70 96:71 97:71 98:72 99:73 100:74 101:75 102:76 103:77 104:78 105:79 106:80 107:80 108:81 109:82 110:83 111:84 112:84 113:85 114:86 115:87 116:87 117:88 118:89 119:90 120:91 121:92 122:93 123:94 124:95 125:96 126:97 127:98 128:98 129:99 130:100 131:101 132:102 133:103 134:104 135:104 136:105 137:106 138:106 139:107 140:108 141:109 142:110 143:111 144:112 145:112 146:113 147:114 148:115 149:116 150:117 151:118 152:119 153:120 154:121 155:121 156:122 157:123 158:124 159:125 160:126 161:127 162:128 163:129 164:129 165:130 166:131 167:131 168:132 169:133 170:134 171:135 172:136 173:137 174:138 175:139 176:140 177:141 178:141 179:142 180:143 181:144 182:145 183:146 184:146 185:147 186:148 187:148 188:148 189:149 190:150 191:151 192:152 193:153 194:154 195:155 196:156 197:156 198:157 199:158 200:159 201:159 202:160 203:161 204:162 205:163 206:164 207:165 208:166 209:167 210:168 211:169 212:170 213:171 214:172 215:173 216:173 217:173 218:174 219:175 220:176 221:177 222:178 223:179 224:180 225:181 226:182 227:183 228:184 229:185 230:186 231:187 232:188 233:189 234:190 235:191 236:192 237:192 238:193 239:194 240:195 241:196 242:196 243:196 244:197 245:198 246:199 247:200 248:201 249:202 250:203 251:204 252:205 253:205 254:206 255:207 256:208 257:209 258:210 259:211 260:212 261:212 262:213 263:214 264:214 265:215 266:216 267:217 268:218 269:219 270:220 271:221 272:222 273:223 274:224 275:225 276:225\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2054 4676 2020 1996 5879 2015 102 1996 5879 5321 2018 1037 2350 2576 1010 3451 1998 2510 4254 2006 5781 2885 1998 2130 1996 2379 2264 1012 1996 5879 2015 2020 15607 2005 2037 7761 4382 1998 2776 2005 2037 3017 11345 3723 1010 3352 16258 21576 2015 1997 1996 3234 26582 2046 2029 2027 4632 27605 13776 1012 2027 4233 1996 25624 1011 7472 2653 1997 1996 26165 2455 2027 3876 1010 2037 9329 3352 2124 2004 5879 1010 20692 8630 2030 5879 2413 1010 2019 2590 4706 2653 1012 1996 11068 1997 13298 1010 2029 2027 2719 2011 5036 2007 1996 2413 4410 1010 2001 1037 2307 10882 12879 1997 5781 2605 1010 1998 2104 2957 1045 1997 13298 2001 16158 2046 1037 2522 21579 1998 18085 18018 1999 16708 7470 1012 1996 5879 2015 2024 3264 2119 2005 2037 3226 1010 2107 2004 2037 4310 17135 4294 1998 3315 7443 1010 1998 2005 2037 3278 2510 17571 1998 15463 1012 5879 29506 2015 2631 1996 2983 1997 12071 2104 5074 2462 2044 16152 2075 2670 3304 2006 1996 7354 19023 1998 8734 2015 1010 1998 2019 5590 2006 6852 1997 2037 3804 1010 2520 1996 25466 1010 2419 2000 1996 5879 9187 1997 2563 2012 1996 2645 1997 12296 1999 10114 2575 1012 5879 3451 1998 2510 3747 3659 2013 2122 2047 2647 8941 2000 1996 25237 2163 1997 1996 2379 2264 1010 2073 2037 3159 8945 29122 15422 1045 2631 1996 18018 1997 19078 1999 1996 24485 1010 2000 3885 1998 3575 1999 2307 3725 1010 2000 3163 1010 1998 2000 1996 20266 1997 2167 3088 1998 1996 17154 3470 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000012\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 12\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] what type of major impact did the norman dynasty have on modern europe ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were famed for their martial spirit and eventually for their christian pie ##ty , becoming expo ##nent ##s of the catholic orthodoxy into which they ass ##imi ##lated . they adopted the gallo - romance language of the frankish land they settled , their dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a co ##hesive and formidable principality in feudal tenure . the norman ##s are noted both for their culture , such as their unique romanesque architecture and musical traditions , and for their significant military accomplishments and innovations . norman adventurer ##s founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the sara ##cens and byzantine ##s , and an expedition on behalf of their duke , william the conqueror , led to the norman conquest of england at the battle of hastings in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where their prince bo ##hem ##ond i founded the principality of antioch in the levant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 16:0 17:1 18:2 19:3 20:4 21:5 22:6 23:6 24:7 25:8 26:9 27:10 28:11 29:12 30:13 31:14 32:15 33:16 34:17 35:18 36:18 37:19 38:20 39:20 40:21 41:22 42:23 43:24 44:25 45:26 46:27 47:28 48:29 49:30 50:31 51:32 52:32 53:32 54:33 55:34 56:34 57:34 58:35 59:36 60:37 61:38 62:39 63:40 64:41 65:42 66:42 67:42 68:42 69:43 70:44 71:45 72:46 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:53 83:54 84:55 85:56 86:57 87:58 88:59 89:59 90:60 91:60 92:61 93:62 94:63 95:63 96:64 97:65 98:66 99:67 100:67 101:68 102:69 103:70 104:71 105:71 106:72 107:73 108:74 109:75 110:76 111:77 112:78 113:79 114:80 115:80 116:81 117:82 118:83 119:84 120:84 121:85 122:86 123:87 124:87 125:88 126:89 127:90 128:91 129:92 130:93 131:94 132:95 133:96 134:97 135:98 136:98 137:99 138:100 139:101 140:102 141:103 142:104 143:104 144:105 145:106 146:106 147:107 148:108 149:109 150:110 151:111 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:121 164:122 165:123 166:124 167:125 168:126 169:127 170:128 171:129 172:129 173:130 174:131 175:131 176:132 177:133 178:134 179:135 180:136 181:137 182:138 183:139 184:140 185:141 186:141 187:142 188:143 189:144 190:145 191:146 192:146 193:147 194:148 195:148 196:148 197:149 198:150 199:151 200:152 201:153 202:154 203:155 204:156 205:156 206:157 207:158 208:159 209:159 210:160 211:161 212:162 213:163 214:164 215:165 216:166 217:167 218:168 219:169 220:170 221:171 222:172 223:173 224:173 225:173 226:174 227:175 228:176 229:177 230:178 231:179 232:180 233:181 234:182 235:183 236:184 237:185 238:186 239:187 240:188 241:189 242:190 243:191 244:192 245:192 246:193 247:194 248:195 249:196 250:196 251:196 252:197 253:198 254:199 255:200 256:201 257:202 258:203 259:204 260:205 261:205 262:206 263:207 264:208 265:209 266:210 267:211 268:212 269:212 270:213 271:214 272:214 273:215 274:216 275:217 276:218 277:219 278:220 279:221 280:222 281:223 282:224 283:225 284:225\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2054 2828 1997 2350 4254 2106 1996 5879 5321 2031 2006 2715 2885 1029 102 1996 5879 5321 2018 1037 2350 2576 1010 3451 1998 2510 4254 2006 5781 2885 1998 2130 1996 2379 2264 1012 1996 5879 2015 2020 15607 2005 2037 7761 4382 1998 2776 2005 2037 3017 11345 3723 1010 3352 16258 21576 2015 1997 1996 3234 26582 2046 2029 2027 4632 27605 13776 1012 2027 4233 1996 25624 1011 7472 2653 1997 1996 26165 2455 2027 3876 1010 2037 9329 3352 2124 2004 5879 1010 20692 8630 2030 5879 2413 1010 2019 2590 4706 2653 1012 1996 11068 1997 13298 1010 2029 2027 2719 2011 5036 2007 1996 2413 4410 1010 2001 1037 2307 10882 12879 1997 5781 2605 1010 1998 2104 2957 1045 1997 13298 2001 16158 2046 1037 2522 21579 1998 18085 18018 1999 16708 7470 1012 1996 5879 2015 2024 3264 2119 2005 2037 3226 1010 2107 2004 2037 4310 17135 4294 1998 3315 7443 1010 1998 2005 2037 3278 2510 17571 1998 15463 1012 5879 29506 2015 2631 1996 2983 1997 12071 2104 5074 2462 2044 16152 2075 2670 3304 2006 1996 7354 19023 1998 8734 2015 1010 1998 2019 5590 2006 6852 1997 2037 3804 1010 2520 1996 25466 1010 2419 2000 1996 5879 9187 1997 2563 2012 1996 2645 1997 12296 1999 10114 2575 1012 5879 3451 1998 2510 3747 3659 2013 2122 2047 2647 8941 2000 1996 25237 2163 1997 1996 2379 2264 1010 2073 2037 3159 8945 29122 15422 1045 2631 1996 18018 1997 19078 1999 1996 24485 1010 2000 3885 1998 3575 1999 2307 3725 1010 2000 3163 1010 1998 2000 1996 20266 1997 2167 3088 1998 1996 17154 3470 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000013\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 13\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] who was famed for their christian spirit ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were famed for their martial spirit and eventually for their christian pie ##ty , becoming expo ##nent ##s of the catholic orthodoxy into which they ass ##imi ##lated . they adopted the gallo - romance language of the frankish land they settled , their dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a co ##hesive and formidable principality in feudal tenure . the norman ##s are noted both for their culture , such as their unique romanesque architecture and musical traditions , and for their significant military accomplishments and innovations . norman adventurer ##s founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the sara ##cens and byzantine ##s , and an expedition on behalf of their duke , william the conqueror , led to the norman conquest of england at the battle of hastings in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where their prince bo ##hem ##ond i founded the principality of antioch in the levant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:18 31:19 32:20 33:20 34:21 35:22 36:23 37:24 38:25 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:32 47:32 48:33 49:34 50:34 51:34 52:35 53:36 54:37 55:38 56:39 57:40 58:41 59:42 60:42 61:42 62:42 63:43 64:44 65:45 66:46 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:55 79:56 80:57 81:58 82:59 83:59 84:60 85:60 86:61 87:62 88:63 89:63 90:64 91:65 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:71 100:72 101:73 102:74 103:75 104:76 105:77 106:78 107:79 108:80 109:80 110:81 111:82 112:83 113:84 114:84 115:85 116:86 117:87 118:87 119:88 120:89 121:90 122:91 123:92 124:93 125:94 126:95 127:96 128:97 129:98 130:98 131:99 132:100 133:101 134:102 135:103 136:104 137:104 138:105 139:106 140:106 141:107 142:108 143:109 144:110 145:111 146:112 147:112 148:113 149:114 150:115 151:116 152:117 153:118 154:119 155:120 156:121 157:121 158:122 159:123 160:124 161:125 162:126 163:127 164:128 165:129 166:129 167:130 168:131 169:131 170:132 171:133 172:134 173:135 174:136 175:137 176:138 177:139 178:140 179:141 180:141 181:142 182:143 183:144 184:145 185:146 186:146 187:147 188:148 189:148 190:148 191:149 192:150 193:151 194:152 195:153 196:154 197:155 198:156 199:156 200:157 201:158 202:159 203:159 204:160 205:161 206:162 207:163 208:164 209:165 210:166 211:167 212:168 213:169 214:170 215:171 216:172 217:173 218:173 219:173 220:174 221:175 222:176 223:177 224:178 225:179 226:180 227:181 228:182 229:183 230:184 231:185 232:186 233:187 234:188 235:189 236:190 237:191 238:192 239:192 240:193 241:194 242:195 243:196 244:196 245:196 246:197 247:198 248:199 249:200 250:201 251:202 252:203 253:204 254:205 255:205 256:206 257:207 258:208 259:209 260:210 261:211 262:212 263:212 264:213 265:214 266:214 267:215 268:216 269:217 270:218 271:219 272:220 273:221 274:222 275:223 276:224 277:225 278:225\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2040 2001 15607 2005 2037 3017 4382 1029 102 1996 5879 5321 2018 1037 2350 2576 1010 3451 1998 2510 4254 2006 5781 2885 1998 2130 1996 2379 2264 1012 1996 5879 2015 2020 15607 2005 2037 7761 4382 1998 2776 2005 2037 3017 11345 3723 1010 3352 16258 21576 2015 1997 1996 3234 26582 2046 2029 2027 4632 27605 13776 1012 2027 4233 1996 25624 1011 7472 2653 1997 1996 26165 2455 2027 3876 1010 2037 9329 3352 2124 2004 5879 1010 20692 8630 2030 5879 2413 1010 2019 2590 4706 2653 1012 1996 11068 1997 13298 1010 2029 2027 2719 2011 5036 2007 1996 2413 4410 1010 2001 1037 2307 10882 12879 1997 5781 2605 1010 1998 2104 2957 1045 1997 13298 2001 16158 2046 1037 2522 21579 1998 18085 18018 1999 16708 7470 1012 1996 5879 2015 2024 3264 2119 2005 2037 3226 1010 2107 2004 2037 4310 17135 4294 1998 3315 7443 1010 1998 2005 2037 3278 2510 17571 1998 15463 1012 5879 29506 2015 2631 1996 2983 1997 12071 2104 5074 2462 2044 16152 2075 2670 3304 2006 1996 7354 19023 1998 8734 2015 1010 1998 2019 5590 2006 6852 1997 2037 3804 1010 2520 1996 25466 1010 2419 2000 1996 5879 9187 1997 2563 2012 1996 2645 1997 12296 1999 10114 2575 1012 5879 3451 1998 2510 3747 3659 2013 2122 2047 2647 8941 2000 1996 25237 2163 1997 1996 2379 2264 1010 2073 2037 3159 8945 29122 15422 1045 2631 1996 18018 1997 19078 1999 1996 24485 1010 2000 3885 1998 3575 1999 2307 3725 1010 2000 3163 1010 1998 2000 1996 20266 1997 2167 3088 1998 1996 17154 3470 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000014\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 14\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] who ass ##imi ##lt ##ed the roman language ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were famed for their martial spirit and eventually for their christian pie ##ty , becoming expo ##nent ##s of the catholic orthodoxy into which they ass ##imi ##lated . they adopted the gallo - romance language of the frankish land they settled , their dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a co ##hesive and formidable principality in feudal tenure . the norman ##s are noted both for their culture , such as their unique romanesque architecture and musical traditions , and for their significant military accomplishments and innovations . norman adventurer ##s founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the sara ##cens and byzantine ##s , and an expedition on behalf of their duke , william the conqueror , led to the norman conquest of england at the battle of hastings in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where their prince bo ##hem ##ond i founded the principality of antioch in the levant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:18 31:18 32:19 33:20 34:20 35:21 36:22 37:23 38:24 39:25 40:26 41:27 42:28 43:29 44:30 45:31 46:32 47:32 48:32 49:33 50:34 51:34 52:34 53:35 54:36 55:37 56:38 57:39 58:40 59:41 60:42 61:42 62:42 63:42 64:43 65:44 66:45 67:46 68:46 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:53 77:53 78:54 79:55 80:56 81:57 82:58 83:59 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:78 108:79 109:80 110:80 111:81 112:82 113:83 114:84 115:84 116:85 117:86 118:87 119:87 120:88 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:98 132:99 133:100 134:101 135:102 136:103 137:104 138:104 139:105 140:106 141:106 142:107 143:108 144:109 145:110 146:111 147:112 148:112 149:113 150:114 151:115 152:116 153:117 154:118 155:119 156:120 157:121 158:121 159:122 160:123 161:124 162:125 163:126 164:127 165:128 166:129 167:129 168:130 169:131 170:131 171:132 172:133 173:134 174:135 175:136 176:137 177:138 178:139 179:140 180:141 181:141 182:142 183:143 184:144 185:145 186:146 187:146 188:147 189:148 190:148 191:148 192:149 193:150 194:151 195:152 196:153 197:154 198:155 199:156 200:156 201:157 202:158 203:159 204:159 205:160 206:161 207:162 208:163 209:164 210:165 211:166 212:167 213:168 214:169 215:170 216:171 217:172 218:173 219:173 220:173 221:174 222:175 223:176 224:177 225:178 226:179 227:180 228:181 229:182 230:183 231:184 232:185 233:186 234:187 235:188 236:189 237:190 238:191 239:192 240:192 241:193 242:194 243:195 244:196 245:196 246:196 247:197 248:198 249:199 250:200 251:201 252:202 253:203 254:204 255:205 256:205 257:206 258:207 259:208 260:209 261:210 262:211 263:212 264:212 265:213 266:214 267:214 268:215 269:216 270:217 271:218 272:219 273:220 274:221 275:222 276:223 277:224 278:225 279:225\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2040 4632 27605 7096 2098 1996 3142 2653 1029 102 1996 5879 5321 2018 1037 2350 2576 1010 3451 1998 2510 4254 2006 5781 2885 1998 2130 1996 2379 2264 1012 1996 5879 2015 2020 15607 2005 2037 7761 4382 1998 2776 2005 2037 3017 11345 3723 1010 3352 16258 21576 2015 1997 1996 3234 26582 2046 2029 2027 4632 27605 13776 1012 2027 4233 1996 25624 1011 7472 2653 1997 1996 26165 2455 2027 3876 1010 2037 9329 3352 2124 2004 5879 1010 20692 8630 2030 5879 2413 1010 2019 2590 4706 2653 1012 1996 11068 1997 13298 1010 2029 2027 2719 2011 5036 2007 1996 2413 4410 1010 2001 1037 2307 10882 12879 1997 5781 2605 1010 1998 2104 2957 1045 1997 13298 2001 16158 2046 1037 2522 21579 1998 18085 18018 1999 16708 7470 1012 1996 5879 2015 2024 3264 2119 2005 2037 3226 1010 2107 2004 2037 4310 17135 4294 1998 3315 7443 1010 1998 2005 2037 3278 2510 17571 1998 15463 1012 5879 29506 2015 2631 1996 2983 1997 12071 2104 5074 2462 2044 16152 2075 2670 3304 2006 1996 7354 19023 1998 8734 2015 1010 1998 2019 5590 2006 6852 1997 2037 3804 1010 2520 1996 25466 1010 2419 2000 1996 5879 9187 1997 2563 2012 1996 2645 1997 12296 1999 10114 2575 1012 5879 3451 1998 2510 3747 3659 2013 2122 2047 2647 8941 2000 1996 25237 2163 1997 1996 2379 2264 1010 2073 2037 3159 8945 29122 15422 1045 2631 1996 18018 1997 19078 1999 1996 24485 1010 2000 3885 1998 3575 1999 2307 3725 1010 2000 3163 1010 1998 2000 1996 20266 1997 2167 3088 1998 1996 17154 3470 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000015\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 15\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] who ruled the country of normandy ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were famed for their martial spirit and eventually for their christian pie ##ty , becoming expo ##nent ##s of the catholic orthodoxy into which they ass ##imi ##lated . they adopted the gallo - romance language of the frankish land they settled , their dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a co ##hesive and formidable principality in feudal tenure . the norman ##s are noted both for their culture , such as their unique romanesque architecture and musical traditions , and for their significant military accomplishments and innovations . norman adventurer ##s founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the sara ##cens and byzantine ##s , and an expedition on behalf of their duke , william the conqueror , led to the norman conquest of england at the battle of hastings in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where their prince bo ##hem ##ond i founded the principality of antioch in the levant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:18 29:18 30:19 31:20 32:20 33:21 34:22 35:23 36:24 37:25 38:26 39:27 40:28 41:29 42:30 43:31 44:32 45:32 46:32 47:33 48:34 49:34 50:34 51:35 52:36 53:37 54:38 55:39 56:40 57:41 58:42 59:42 60:42 61:42 62:43 63:44 64:45 65:46 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:55 78:56 79:57 80:58 81:59 82:59 83:60 84:60 85:61 86:62 87:63 88:63 89:64 90:65 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:71 99:72 100:73 101:74 102:75 103:76 104:77 105:78 106:79 107:80 108:80 109:81 110:82 111:83 112:84 113:84 114:85 115:86 116:87 117:87 118:88 119:89 120:90 121:91 122:92 123:93 124:94 125:95 126:96 127:97 128:98 129:98 130:99 131:100 132:101 133:102 134:103 135:104 136:104 137:105 138:106 139:106 140:107 141:108 142:109 143:110 144:111 145:112 146:112 147:113 148:114 149:115 150:116 151:117 152:118 153:119 154:120 155:121 156:121 157:122 158:123 159:124 160:125 161:126 162:127 163:128 164:129 165:129 166:130 167:131 168:131 169:132 170:133 171:134 172:135 173:136 174:137 175:138 176:139 177:140 178:141 179:141 180:142 181:143 182:144 183:145 184:146 185:146 186:147 187:148 188:148 189:148 190:149 191:150 192:151 193:152 194:153 195:154 196:155 197:156 198:156 199:157 200:158 201:159 202:159 203:160 204:161 205:162 206:163 207:164 208:165 209:166 210:167 211:168 212:169 213:170 214:171 215:172 216:173 217:173 218:173 219:174 220:175 221:176 222:177 223:178 224:179 225:180 226:181 227:182 228:183 229:184 230:185 231:186 232:187 233:188 234:189 235:190 236:191 237:192 238:192 239:193 240:194 241:195 242:196 243:196 244:196 245:197 246:198 247:199 248:200 249:201 250:202 251:203 252:204 253:205 254:205 255:206 256:207 257:208 258:209 259:210 260:211 261:212 262:212 263:213 264:214 265:214 266:215 267:216 268:217 269:218 270:219 271:220 272:221 273:222 274:223 275:224 276:225 277:225\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2040 5451 1996 2406 1997 13298 1029 102 1996 5879 5321 2018 1037 2350 2576 1010 3451 1998 2510 4254 2006 5781 2885 1998 2130 1996 2379 2264 1012 1996 5879 2015 2020 15607 2005 2037 7761 4382 1998 2776 2005 2037 3017 11345 3723 1010 3352 16258 21576 2015 1997 1996 3234 26582 2046 2029 2027 4632 27605 13776 1012 2027 4233 1996 25624 1011 7472 2653 1997 1996 26165 2455 2027 3876 1010 2037 9329 3352 2124 2004 5879 1010 20692 8630 2030 5879 2413 1010 2019 2590 4706 2653 1012 1996 11068 1997 13298 1010 2029 2027 2719 2011 5036 2007 1996 2413 4410 1010 2001 1037 2307 10882 12879 1997 5781 2605 1010 1998 2104 2957 1045 1997 13298 2001 16158 2046 1037 2522 21579 1998 18085 18018 1999 16708 7470 1012 1996 5879 2015 2024 3264 2119 2005 2037 3226 1010 2107 2004 2037 4310 17135 4294 1998 3315 7443 1010 1998 2005 2037 3278 2510 17571 1998 15463 1012 5879 29506 2015 2631 1996 2983 1997 12071 2104 5074 2462 2044 16152 2075 2670 3304 2006 1996 7354 19023 1998 8734 2015 1010 1998 2019 5590 2006 6852 1997 2037 3804 1010 2520 1996 25466 1010 2419 2000 1996 5879 9187 1997 2563 2012 1996 2645 1997 12296 1999 10114 2575 1012 5879 3451 1998 2510 3747 3659 2013 2122 2047 2647 8941 2000 1996 25237 2163 1997 1996 2379 2264 1010 2073 2037 3159 8945 29122 15422 1045 2631 1996 18018 1997 19078 1999 1996 24485 1010 2000 3885 1998 3575 1999 2307 3725 1010 2000 3163 1010 1998 2000 1996 20266 1997 2167 3088 1998 1996 17154 3470 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000016\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 16\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] what principality did william the conquer ##er found ? [SEP] the norman dynasty had a major political , cultural and military impact on medieval europe and even the near east . the norman ##s were famed for their martial spirit and eventually for their christian pie ##ty , becoming expo ##nent ##s of the catholic orthodoxy into which they ass ##imi ##lated . they adopted the gallo - romance language of the frankish land they settled , their dialect becoming known as norman , norma ##und or norman french , an important literary language . the duchy of normandy , which they formed by treaty with the french crown , was a great fi ##ef of medieval france , and under richard i of normandy was forged into a co ##hesive and formidable principality in feudal tenure . the norman ##s are noted both for their culture , such as their unique romanesque architecture and musical traditions , and for their significant military accomplishments and innovations . norman adventurer ##s founded the kingdom of sicily under roger ii after conquer ##ing southern italy on the sara ##cens and byzantine ##s , and an expedition on behalf of their duke , william the conqueror , led to the norman conquest of england at the battle of hastings in 106 ##6 . norman cultural and military influence spread from these new european centres to the crusader states of the near east , where their prince bo ##hem ##ond i founded the principality of antioch in the levant , to scotland and wales in great britain , to ireland , and to the coasts of north africa and the canary islands . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:18 31:18 32:19 33:20 34:20 35:21 36:22 37:23 38:24 39:25 40:26 41:27 42:28 43:29 44:30 45:31 46:32 47:32 48:32 49:33 50:34 51:34 52:34 53:35 54:36 55:37 56:38 57:39 58:40 59:41 60:42 61:42 62:42 63:42 64:43 65:44 66:45 67:46 68:46 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:53 77:53 78:54 79:55 80:56 81:57 82:58 83:59 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:78 108:79 109:80 110:80 111:81 112:82 113:83 114:84 115:84 116:85 117:86 118:87 119:87 120:88 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:98 132:99 133:100 134:101 135:102 136:103 137:104 138:104 139:105 140:106 141:106 142:107 143:108 144:109 145:110 146:111 147:112 148:112 149:113 150:114 151:115 152:116 153:117 154:118 155:119 156:120 157:121 158:121 159:122 160:123 161:124 162:125 163:126 164:127 165:128 166:129 167:129 168:130 169:131 170:131 171:132 172:133 173:134 174:135 175:136 176:137 177:138 178:139 179:140 180:141 181:141 182:142 183:143 184:144 185:145 186:146 187:146 188:147 189:148 190:148 191:148 192:149 193:150 194:151 195:152 196:153 197:154 198:155 199:156 200:156 201:157 202:158 203:159 204:159 205:160 206:161 207:162 208:163 209:164 210:165 211:166 212:167 213:168 214:169 215:170 216:171 217:172 218:173 219:173 220:173 221:174 222:175 223:176 224:177 225:178 226:179 227:180 228:181 229:182 230:183 231:184 232:185 233:186 234:187 235:188 236:189 237:190 238:191 239:192 240:192 241:193 242:194 243:195 244:196 245:196 246:196 247:197 248:198 249:199 250:200 251:201 252:202 253:203 254:204 255:205 256:205 257:206 258:207 259:208 260:209 261:210 262:211 263:212 264:212 265:213 266:214 267:214 268:215 269:216 270:217 271:218 272:219 273:220 274:221 275:222 276:223 277:224 278:225 279:225\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2054 18018 2106 2520 1996 16152 2121 2179 1029 102 1996 5879 5321 2018 1037 2350 2576 1010 3451 1998 2510 4254 2006 5781 2885 1998 2130 1996 2379 2264 1012 1996 5879 2015 2020 15607 2005 2037 7761 4382 1998 2776 2005 2037 3017 11345 3723 1010 3352 16258 21576 2015 1997 1996 3234 26582 2046 2029 2027 4632 27605 13776 1012 2027 4233 1996 25624 1011 7472 2653 1997 1996 26165 2455 2027 3876 1010 2037 9329 3352 2124 2004 5879 1010 20692 8630 2030 5879 2413 1010 2019 2590 4706 2653 1012 1996 11068 1997 13298 1010 2029 2027 2719 2011 5036 2007 1996 2413 4410 1010 2001 1037 2307 10882 12879 1997 5781 2605 1010 1998 2104 2957 1045 1997 13298 2001 16158 2046 1037 2522 21579 1998 18085 18018 1999 16708 7470 1012 1996 5879 2015 2024 3264 2119 2005 2037 3226 1010 2107 2004 2037 4310 17135 4294 1998 3315 7443 1010 1998 2005 2037 3278 2510 17571 1998 15463 1012 5879 29506 2015 2631 1996 2983 1997 12071 2104 5074 2462 2044 16152 2075 2670 3304 2006 1996 7354 19023 1998 8734 2015 1010 1998 2019 5590 2006 6852 1997 2037 3804 1010 2520 1996 25466 1010 2419 2000 1996 5879 9187 1997 2563 2012 1996 2645 1997 12296 1999 10114 2575 1012 5879 3451 1998 2510 3747 3659 2013 2122 2047 2647 8941 2000 1996 25237 2163 1997 1996 2379 2264 1010 2073 2037 3159 8945 29122 15422 1045 2631 1996 18018 1997 19078 1999 1996 24485 1010 2000 3885 1998 3575 1999 2307 3725 1010 2000 3163 1010 1998 2000 1996 20266 1997 2167 3088 1998 1996 17154 3470 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "\r",
      "convertting examples to features:   0%|     | 17/11873 [00:00<01:15, 157.80it/s]08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000017\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 17\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] what is the original meaning of the word norman ? [SEP] the english name \" norman ##s \" comes from the french words norman ##s / norman ##z , plural of norman ##t , modern french norman ##d , which is itself borrowed from old low franco ##nian nor ##tman ##n \" north ##man \" or directly from old norse nor ##ð ##ma ##ð ##r , latin ##ized variously as nor ##tman ##nus , norman ##nus , or nord ##mann ##us ( recorded in medieval latin , 9th century ) to mean \" norse ##man , viking \" . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 12:0 13:1 14:2 15:3 16:3 17:3 18:3 19:4 20:5 21:6 22:7 23:8 24:9 25:9 26:9 27:9 28:9 29:9 30:10 31:11 32:12 33:12 34:12 35:13 36:14 37:15 38:15 39:15 40:16 41:17 42:18 43:19 44:20 45:21 46:22 47:23 48:23 49:24 50:24 51:24 52:25 53:25 54:25 55:25 56:26 57:27 58:28 59:29 60:30 61:31 62:31 63:31 64:31 65:31 66:31 67:32 68:32 69:33 70:34 71:35 72:35 73:35 74:35 75:36 76:36 77:36 78:37 79:38 80:38 81:38 82:39 83:39 84:40 85:41 86:42 87:42 88:43 89:44 90:44 91:45 92:46 93:47 94:47 95:47 96:47 97:48 98:48 99:48\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2054 2003 1996 2434 3574 1997 1996 2773 5879 1029 102 1996 2394 2171 1000 5879 2015 1000 3310 2013 1996 2413 2616 5879 2015 1013 5879 2480 1010 13994 1997 5879 2102 1010 2715 2413 5879 2094 1010 2029 2003 2993 11780 2013 2214 2659 9341 11148 4496 22942 2078 1000 2167 2386 1000 2030 3495 2013 2214 15342 4496 29668 2863 29668 2099 1010 3763 3550 17611 2004 4496 22942 10182 1010 5879 10182 1010 2030 13926 5804 2271 1006 2680 1999 5781 3763 1010 6280 2301 1007 2000 2812 1000 15342 2386 1010 12886 1000 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000018\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 18\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] when was the latin version of the word norman first recorded ? [SEP] the english name \" norman ##s \" comes from the french words norman ##s / norman ##z , plural of norman ##t , modern french norman ##d , which is itself borrowed from old low franco ##nian nor ##tman ##n \" north ##man \" or directly from old norse nor ##ð ##ma ##ð ##r , latin ##ized variously as nor ##tman ##nus , norman ##nus , or nord ##mann ##us ( recorded in medieval latin , 9th century ) to mean \" norse ##man , viking \" . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 14:0 15:1 16:2 17:3 18:3 19:3 20:3 21:4 22:5 23:6 24:7 25:8 26:9 27:9 28:9 29:9 30:9 31:9 32:10 33:11 34:12 35:12 36:12 37:13 38:14 39:15 40:15 41:15 42:16 43:17 44:18 45:19 46:20 47:21 48:22 49:23 50:23 51:24 52:24 53:24 54:25 55:25 56:25 57:25 58:26 59:27 60:28 61:29 62:30 63:31 64:31 65:31 66:31 67:31 68:31 69:32 70:32 71:33 72:34 73:35 74:35 75:35 76:35 77:36 78:36 79:36 80:37 81:38 82:38 83:38 84:39 85:39 86:40 87:41 88:42 89:42 90:43 91:44 92:44 93:45 94:46 95:47 96:47 97:47 98:47 99:48 100:48 101:48\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2043 2001 1996 3763 2544 1997 1996 2773 5879 2034 2680 1029 102 1996 2394 2171 1000 5879 2015 1000 3310 2013 1996 2413 2616 5879 2015 1013 5879 2480 1010 13994 1997 5879 2102 1010 2715 2413 5879 2094 1010 2029 2003 2993 11780 2013 2214 2659 9341 11148 4496 22942 2078 1000 2167 2386 1000 2030 3495 2013 2214 15342 4496 29668 2863 29668 2099 1010 3763 3550 17611 2004 4496 22942 10182 1010 5879 10182 1010 2030 13926 5804 2271 1006 2680 1999 5781 3763 1010 6280 2301 1007 2000 2812 1000 15342 2386 1010 12886 1000 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   *** Example ***\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   unique_id: 1000000019\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   example_index: 19\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   doc_span_index: 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   tokens: [CLS] what name comes from the english words norman ##s / norman ##z ? [SEP] the english name \" norman ##s \" comes from the french words norman ##s / norman ##z , plural of norman ##t , modern french norman ##d , which is itself borrowed from old low franco ##nian nor ##tman ##n \" north ##man \" or directly from old norse nor ##ð ##ma ##ð ##r , latin ##ized variously as nor ##tman ##nus , norman ##nus , or nord ##mann ##us ( recorded in medieval latin , 9th century ) to mean \" norse ##man , viking \" . [SEP]\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_to_orig_map: 15:0 16:1 17:2 18:3 19:3 20:3 21:3 22:4 23:5 24:6 25:7 26:8 27:9 28:9 29:9 30:9 31:9 32:9 33:10 34:11 35:12 36:12 37:12 38:13 39:14 40:15 41:15 42:15 43:16 44:17 45:18 46:19 47:20 48:21 49:22 50:23 51:23 52:24 53:24 54:24 55:25 56:25 57:25 58:25 59:26 60:27 61:28 62:29 63:30 64:31 65:31 66:31 67:31 68:31 69:31 70:32 71:32 72:33 73:34 74:35 75:35 76:35 77:35 78:36 79:36 80:36 81:37 82:38 83:38 84:38 85:39 86:39 87:40 88:41 89:42 90:42 91:43 92:44 93:44 94:45 95:46 96:47 97:47 98:47 99:47 100:48 101:48 102:48\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_ids: 101 2054 2171 3310 2013 1996 2394 2616 5879 2015 1013 5879 2480 1029 102 1996 2394 2171 1000 5879 2015 1000 3310 2013 1996 2413 2616 5879 2015 1013 5879 2480 1010 13994 1997 5879 2102 1010 2715 2413 5879 2094 1010 2029 2003 2993 11780 2013 2214 2659 9341 11148 4496 22942 2078 1000 2167 2386 1000 2030 3495 2013 2214 15342 4496 29668 2863 29668 2099 1010 3763 3550 17611 2004 4496 22942 10182 1010 5879 10182 1010 2030 13926 5804 2271 1006 2680 1999 5781 3763 1010 6280 2301 1007 2000 2812 1000 15342 2386 1010 12886 1000 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      "08/13/2022 22:37:45 - INFO - utils_squad -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convertting examples to features: 100%|██| 11873/11873 [00:47<00:00, 250.28it/s]\n",
      "08/13/2022 22:38:33 - INFO - __main__ -   Saving features into cached file squad/cached_dev_bert-base-uncased_384_features\n",
      "08/13/2022 22:38:39 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "08/13/2022 22:38:39 - INFO - __main__ -     Num examples = 12232\n",
      "08/13/2022 22:38:39 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100%|███████████████████████████| 1529/1529 [24:46<00:00,  1.03it/s]\n",
      "08/13/2022 23:03:26 - INFO - utils_squad -   Writing predictions to: outputs/predictions_.json\n",
      "08/13/2022 23:03:26 - INFO - utils_squad -   Writing nbest to: outputs/nbest_predictions_.json\n",
      "examples: 11873\n",
      "features: 12232\n",
      "all_results: 12232\n"
     ]
    }
   ],
   "source": [
    "!python run_evaluate.py --cache_dir=squad --version_2_with_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a924c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"exact\": 23.01861366124821,\r\n",
      "  \"f1\": 29.224821740750045,\r\n",
      "  \"total\": 11873,\r\n",
      "  \"HasAns_exact\": 45.49595141700405,\r\n",
      "  \"HasAns_f1\": 57.92616540619522,\r\n",
      "  \"HasAns_total\": 5928,\r\n",
      "  \"NoAns_exact\": 0.6055508830950378,\r\n",
      "  \"NoAns_f1\": 0.6055508830950378,\r\n",
      "  \"NoAns_total\": 5945\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py squad/dev-v2.0.json outputs/predictions_.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6b291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
